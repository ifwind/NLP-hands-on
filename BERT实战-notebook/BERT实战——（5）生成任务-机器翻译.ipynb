{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "# BERTå®æˆ˜â€”â€”ï¼ˆ5ï¼‰ç”Ÿæˆä»»åŠ¡-æœºå™¨ç¿»è¯‘\n",
    "\n",
    "## å¼•è¨€\n",
    "\n",
    "ä¹‹å‰çš„åˆ†åˆ«ä»‹ç»äº†ä½¿ç”¨ [ğŸ¤— Transformers](https://github.com/huggingface/transformers)ä»£ç åº“ä¸­çš„æ¨¡å‹å¼€å±•one-classä»»åŠ¡([æ–‡æœ¬åˆ†ç±»](https://ifwind.github.io/2021/08/26/BERT%E5%AE%9E%E6%88%98%E2%80%94%E2%80%94%EF%BC%881%EF%BC%89%E6%96%87%E6%9C%AC%E5%88%86%E7%B1%BB/)ã€[å¤šé€‰é—®ç­”é—®é¢˜](https://ifwind.github.io/2021/08/27/BERT%E5%AE%9E%E6%88%98%E2%80%94%E2%80%94%EF%BC%883%EF%BC%89%E9%97%AE%E7%AD%94%E4%BB%BB%E5%8A%A1-%E5%A4%9A%E9%80%89%E9%97%AE%E7%AD%94/))ã€class for each tokenä»»åŠ¡([åºåˆ—æ ‡æ³¨](https://ifwind.github.io/2021/08/27/BERT%E5%AE%9E%E6%88%98%E2%80%94%E2%80%94%EF%BC%882%EF%BC%89%E5%BA%8F%E5%88%97%E6%A0%87%E6%B3%A8/))ä»¥åŠcopy from inputä»»åŠ¡([æŠ½å–å¼é—®ç­”](https://ifwind.github.io/2021/08/30/BERT%E5%AE%9E%E6%88%98%E2%80%94%E2%80%94%EF%BC%884%EF%BC%89%E9%97%AE%E7%AD%94%E4%BB%BB%E5%8A%A1-%E6%8A%BD%E5%8F%96%E5%BC%8F%E9%97%AE%E7%AD%94/))ã€‚\n",
    "\n",
    "è¿™ä¸€ç¯‡ä»¥åŠä¸‹ä¸€ç¯‡å°†ä»‹ç»å¦‚ä½•ä½¿ç”¨ [ğŸ¤— Transformers](https://github.com/huggingface/transformers)ä»£ç åº“ä¸­çš„æ¨¡å‹æ¥**è§£å†³general sequenceä»»åŠ¡**ï¼ˆå…³äºä»€ä¹ˆæ˜¯ç”Ÿæˆåºåˆ—ä»»åŠ¡ï¼Œå›çœ‹[ä¹‹å‰çš„åšå®¢ï¼Œå®šä½è¯ï¼šgeneral sequence](https://ifwind.github.io/2021/08/24/BERT%E7%9B%B8%E5%85%B3%E2%80%94%E2%80%94%EF%BC%887%EF%BC%89%E6%8A%8ABERT%E5%BA%94%E7%94%A8%E5%88%B0%E4%B8%8B%E6%B8%B8%E4%BB%BB%E5%8A%A1/)ï¼‰ã€‚è¿™ä¸€ç¯‡ä¸ºè§£å†³**ç”Ÿæˆä»»åŠ¡ä¸­çš„æœºå™¨ç¿»è¯‘é—®é¢˜**ã€‚\n",
    "\n",
    "### ä»»åŠ¡ä»‹ç»\n",
    "\n",
    "ç¿»è¯‘ä»»åŠ¡ï¼ŒæŠŠä¸€ç§è¯­è¨€ä¿¡æ¯è½¬å˜æˆå¦ä¸€ç§è¯­è¨€ä¿¡æ¯ã€‚æ˜¯å…¸å‹çš„seq2seqä»»åŠ¡ï¼Œè¾“å…¥ä¸ºä¸€ä¸ªåºåˆ—ï¼Œè¾“å‡ºä¸ºä¸å›ºå®šé•¿åº¦ï¼ˆç”±æœºå™¨è‡ªè¡Œå­¦ä¹ ç”Ÿæˆçš„åºåˆ—åº”è¯¥å¤šé•¿ï¼‰çš„åºåˆ—ã€‚\n",
    "\n",
    "æ¯”å¦‚è¾“å…¥ä¸€å¥ä¸­æ–‡ï¼Œç¿»è¯‘ä¸ºè‹±æ–‡ï¼š"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "è¾“å…¥ï¼šæˆ‘çˆ±ä¸­å›½ã€‚\n",
    "è¾“å‡ºï¼šI love China."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "ä¸»è¦åˆ†ä¸ºä»¥ä¸‹å‡ ä¸ªéƒ¨åˆ†ï¼š\n",
    "\n",
    "1. æ•°æ®åŠ è½½ï¼›\n",
    "2. æ•°æ®é¢„å¤„ç†ï¼›\n",
    "3. å¾®è°ƒé¢„è®­ç»ƒæ¨¡å‹ï¼šä½¿ç”¨transformerä¸­çš„**`Seq2SeqTrainer`æ¥å£**å¯¹é¢„è®­ç»ƒæ¨¡å‹è¿›è¡Œå¾®è°ƒï¼ˆæ³¨æ„è¿™é‡Œæ˜¯`Seq2SeqTrainer`æ¥å£ï¼Œä¹‹å‰çš„ä»»åŠ¡éƒ½æ˜¯è°ƒç”¨`Trainer`æ¥å£ï¼‰ã€‚\n",
    "\n",
    "### å‰æœŸå‡†å¤‡\n",
    "\n",
    "å®‰è£…ä»¥ä¸‹åº“ï¼š"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pip install datasets transformers sacrebleu sentencepiece\n",
    "#transformers==4.9.2\n",
    "#datasets==1.11.0\n",
    "#sacrebleu==1.5.1\n",
    "#sentencepiece==0.1.96"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## æ•°æ®åŠ è½½\n",
    "\n",
    "### æ•°æ®é›†ä»‹ç»\n",
    "\n",
    "æˆ‘ä»¬ä½¿ç”¨[WMT dataset](http://www.statmt.org/wmt16/)æ•°æ®é›†ã€‚è¿™æ˜¯ç¿»è¯‘ä»»åŠ¡æœ€å¸¸ç”¨çš„æ•°æ®é›†ä¹‹ä¸€ã€‚å…¶ä¸­åŒ…æ‹¬English/RomanianåŒè¯­ç¿»è¯‘ã€‚\n",
    "\n",
    "### åŠ è½½æ•°æ®\n",
    "\n",
    "è¯¥æ•°æ®çš„åŠ è½½æ–¹å¼åœ¨transformersåº“ä¸­è¿›è¡Œäº†å°è£…ï¼Œæˆ‘ä»¬å¯ä»¥é€šè¿‡ä»¥ä¸‹è¯­å¥è¿›è¡Œæ•°æ®åŠ è½½ï¼š"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from datasets import load_dataset\n",
    "raw_datasets = load_dataset(\"wmt16\", \"ro-en\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "ç»™å®šä¸€ä¸ªæ•°æ®åˆ‡åˆ†çš„keyï¼ˆtrainã€validationæˆ–è€…testï¼‰å’Œä¸‹æ ‡å³å¯æŸ¥çœ‹æ•°æ®ã€‚"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "raw_datasets[\"train\"][0]\n",
    "# æˆ‘ä»¬å¯ä»¥çœ‹åˆ°ä¸€å¥è‹±è¯­enå¯¹åº”ä¸€å¥ç½—é©¬å°¼äºšè¯­è¨€ro\n",
    "# {'translation': {'en': 'Membership of Parliament: see Minutes','ro': 'ComponenÅ£a Parlamentului: a se vedea procesul-verbal'}}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "ä¸‹é¢çš„å‡½æ•°å°†ä»æ•°æ®é›†é‡Œéšæœºé€‰æ‹©å‡ ä¸ªä¾‹å­è¿›è¡Œå±•ç¤ºï¼š"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import datasets\n",
    "import random\n",
    "import pandas as pd\n",
    "from IPython.display import display, HTML\n",
    "\n",
    "def show_random_elements(dataset, num_examples=5):\n",
    "    assert num_examples <= len(dataset), \"Can't pick more elements than there are in the dataset.\"\n",
    "    picks = []\n",
    "    for _ in range(num_examples):\n",
    "        pick = random.randint(0, len(dataset)-1)\n",
    "        while pick in picks:\n",
    "            pick = random.randint(0, len(dataset)-1)\n",
    "        picks.append(pick)\n",
    "    \n",
    "    df = pd.DataFrame(dataset[picks])\n",
    "    for column, typ in dataset.features.items():\n",
    "        if isinstance(typ, datasets.ClassLabel):\n",
    "            df[column] = df[column].transform(lambda i: typ.names[i])\n",
    "    display(HTML(df.to_html()))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "show_random_elements(raw_datasets[\"train\"])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<table border=\"1\" class=\"dataframe\">\n",
    "  <thead>\n",
    "    <tr style=\"text-align: right;\">\n",
    "      <th></th>\n",
    "      <th>translation</th>\n",
    "    </tr>\n",
    "  </thead>\n",
    "  <tbody>\n",
    "    <tr>\n",
    "      <th>0</th>\n",
    "      <td>{'en': 'The Bulgarian gymnastics team won the gold medal at the traditional Grand Prix series competition in Thiais, France, which wrapped up on Sunday (March 30th).', 'ro': 'Echipa bulgarÄƒ de gimnasticÄƒ a cÃ¢ÅŸtigat medalia de aur la tradiÅ£ionala competiÅ£ie Grand Prix din Thiais, FranÅ£a, care s-a Ã®ncheiat duminicÄƒ (30 martie).'}</td>\n",
    "    </tr>\n",
    "    <tr>\n",
    "      <th>1</th>\n",
    "      <td>{'en': 'Being on that committee, however, you will know that this was a very hot topic in negotiations between Norway and some Member States.', 'ro': 'TotuÅŸi, fÄƒcÃ¢nd parte din aceastÄƒ comisie, ÅŸtiÅ£i cÄƒ acesta a fost un subiect foarte aprins Ã®n negocierile dintre Norvegia ÅŸi unele state membre.'}</td>\n",
    "    </tr>\n",
    "    <tr>\n",
    "      <th>2</th>\n",
    "      <td>{'en': 'The overwhelming vote shows just this.', 'ro': 'Ceea ce demonstreazÄƒ ÅŸi votul favorabil.'}</td>\n",
    "    </tr>\n",
    "    <tr>\n",
    "      <th>3</th>\n",
    "      <td>{'en': '[Photo illustration by Catherine Gurgenidze for Southeast European Times]', 'ro': '[IlustraÅ£ii foto de Catherine Gurgenidze pentru Southeast European Times]'}</td>\n",
    "    </tr>\n",
    "    <tr>\n",
    "      <th>4</th>\n",
    "      <td>{'en': '(HU) Mr President, today the specific text of the agreement between the Hungarian Government and the European Commission has been formulated.', 'ro': '(HU) Domnule preÈ™edinte, textul concret al acordului dintre guvernul ungar È™i Comisia EuropeanÄƒ a fost formulat astÄƒzi.'}</td>\n",
    "    </tr>\n",
    "  </tbody>\n",
    "</table>\n",
    "\n",
    "## æ•°æ®é¢„å¤„ç†\n",
    "\n",
    "åœ¨å°†æ•°æ®å–‚å…¥æ¨¡å‹ä¹‹å‰ï¼Œæˆ‘ä»¬éœ€è¦å¯¹æ•°æ®è¿›è¡Œé¢„å¤„ç†ã€‚\n",
    "\n",
    "ä»ç„¶æ˜¯ä¸¤ä¸ªæ•°æ®é¢„å¤„ç†çš„åŸºæœ¬æµç¨‹ï¼š\n",
    "\n",
    "1. åˆ†è¯ï¼›\n",
    "2. è½¬åŒ–æˆå¯¹åº”ä»»åŠ¡è¾“å…¥æ¨¡å‹çš„æ ¼å¼ï¼›\n",
    "\n",
    "`Tokenizer`ç”¨äºä¸Šé¢ä¸¤æ­¥æ•°æ®é¢„å¤„ç†å·¥ä½œï¼š`Tokenizer`é¦–å…ˆå¯¹è¾“å…¥è¿›è¡Œtokenizeï¼Œç„¶åå°†tokensè½¬åŒ–ä¸ºé¢„æ¨¡å‹ä¸­éœ€è¦å¯¹åº”çš„token IDï¼Œå†è½¬åŒ–ä¸ºæ¨¡å‹éœ€è¦çš„è¾“å…¥æ ¼å¼ã€‚\n",
    "\n",
    "### åˆå§‹åŒ–Tokenizer\n",
    "\n",
    "[ä¹‹å‰çš„åšå®¢](https://ifwind.github.io/2021/08/26/BERT%E5%AE%9E%E6%88%98%E2%80%94%E2%80%94%EF%BC%881%EF%BC%89%E6%96%87%E6%9C%AC%E5%88%86%E7%B1%BB/#%E5%88%9D%E5%A7%8B%E5%8C%96Tokenizer)å·²ç»ä»‹ç»äº†ä¸€äº›Tokenizerçš„å†…å®¹ï¼Œå¹¶åšäº†Tokenizeråˆ†è¯çš„ç¤ºä¾‹ï¼Œè¿™é‡Œä¸å†é‡å¤ã€‚`use_fast=True`æŒ‡å®šä½¿ç”¨fastç‰ˆæœ¬çš„tokenizerã€‚æˆ‘ä»¬ä½¿ç”¨å·²ç»è®­ç»ƒå¥½çš„[`Helsinki-NLP/opus-mt-en-ro`](https://huggingface.co/Helsinki-NLP/opus-mt-en-ro) checkpointæ¥åšç¿»è¯‘ä»»åŠ¡ã€‚"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import AutoTokenizer\n",
    "model_checkpoint = \"Helsinki-NLP/opus-mt-en-ro\" \n",
    "tokenizer = AutoTokenizer.from_pretrained(model_checkpoint, use_fast=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "ä»¥ä½¿ç”¨çš„mBARTæ¨¡å‹ä¸ºä¾‹ï¼Œ**éœ€è¦æ­£ç¡®è®¾ç½®sourceè¯­è¨€å’Œtargetè¯­è¨€**ã€‚å¦‚æœç¿»è¯‘çš„æ˜¯å…¶ä»–åŒè¯­è¯­æ–™ï¼Œè¯·æŸ¥çœ‹[è¿™é‡Œ](https://huggingface.co/facebook/mbart-large-cc25)è¿›è¡Œé…ç½®ï¼š"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "if \"mbart\" in model_checkpoint:\n",
    "    tokenizer.src_lang = \"en-XX\"\n",
    "    tokenizer.tgt_lang = \"ro-RO\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### è½¬åŒ–æˆå¯¹åº”ä»»åŠ¡è¾“å…¥æ¨¡å‹çš„æ ¼å¼\n",
    "\n",
    "æ¨¡å‹çš„è¾“å…¥ä¸ºå¾…ç¿»è¯‘çš„å¥å­ã€‚\n",
    "\n",
    "**æ³¨æ„ï¼šä¸ºäº†ç»™æ¨¡å‹å‡†å¤‡å¥½ç¿»è¯‘çš„targetsï¼Œä½¿ç”¨`as_target_tokenizer`æ¥ä¸ºtargetsè®¾ç½®tokenizerï¼š**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "with tokenizer.as_target_tokenizer():\n",
    "    print(tokenizer(\"Hello, this one sentence!\"))\n",
    "    model_input = tokenizer(\"Hello, this one sentence!\")\n",
    "    tokens = tokenizer.convert_ids_to_tokens(model_input['input_ids'])\n",
    "    # æ‰“å°çœ‹ä¸€ä¸‹special toke\n",
    "    print('tokens: {}'.format(tokens))\n",
    "#{'input_ids': [10334, 1204, 3, 15, 8915, 27, 452, 59, 29579, 581, 23, 0], 'attention_mask': [1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1]}\n",
    "#tokens: ['â–Hel', 'lo', ',', 'â–', 'this', 'â–o', 'ne', 'â–se', 'nten', 'ce', '!', '</s>']"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**å¦‚æœä½¿ç”¨çš„æ˜¯T5é¢„è®­ç»ƒæ¨¡å‹çš„checkpointsï¼Œéœ€è¦å¯¹ç‰¹æ®Šçš„å‰ç¼€è¿›è¡Œæ£€æŸ¥ã€‚T5ä½¿ç”¨ç‰¹æ®Šçš„å‰ç¼€æ¥å‘Šè¯‰æ¨¡å‹å…·ä½“è¦åšçš„ä»»åŠ¡ï¼ˆ`\"translate English to Romanian: \"`ï¼‰**ï¼Œå…·ä½“å‰ç¼€ä¾‹å­å¦‚ä¸‹ï¼š"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "if model_checkpoint in [\"t5-small\", \"t5-base\", \"t5-larg\", \"t5-3b\", \"t5-11b\"]:\n",
    "    prefix = \"translate English to Romanian: \"\n",
    "else:\n",
    "    prefix = \"\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "ç°åœ¨æˆ‘ä»¬å¯ä»¥æŠŠä¸Šé¢çš„å†…å®¹æ”¾åœ¨ä¸€èµ·ç»„æˆé¢„å¤„ç†å‡½æ•°`preprocess_function`ã€‚å¯¹æ ·æœ¬è¿›è¡Œé¢„å¤„ç†çš„æ—¶å€™ï¼Œ**ä½¿ç”¨`truncation=True`å‚æ•°æ¥ç¡®ä¿è¶…é•¿æ–‡æœ¬è¢«æˆªæ–­ã€‚é»˜è®¤æƒ…å†µä¸‹ï¼Œå¯¹ä¸æ¯”è¾ƒçŸ­çš„å¥å­ä¼šè‡ªåŠ¨paddingã€‚**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "max_input_length = 128\n",
    "max_target_length = 128\n",
    "source_lang = \"en\"\n",
    "target_lang = \"ro\"\n",
    "\n",
    "def preprocess_function(examples):\n",
    "    inputs = [prefix + ex[source_lang] for ex in examples[\"translation\"]]\n",
    "    targets = [ex[target_lang] for ex in examples[\"translation\"]]\n",
    "    model_inputs = tokenizer(inputs, max_length=max_input_length, truncation=True)\n",
    "\n",
    "    # Setup the tokenizer for targets\n",
    "    with tokenizer.as_target_tokenizer():\n",
    "        labels = tokenizer(targets, max_length=max_target_length, truncation=True)\n",
    "\n",
    "    model_inputs[\"labels\"] = labels[\"input_ids\"]\n",
    "    return model_inputs"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "ä»¥ä¸Šçš„é¢„å¤„ç†å‡½æ•°å¯ä»¥å¤„ç†ä¸€ä¸ªæ ·æœ¬ï¼Œä¹Ÿå¯ä»¥å¤„ç†å¤šä¸ªæ ·æœ¬exapmlesã€‚å¦‚æœæ˜¯å¤„ç†å¤šä¸ªæ ·æœ¬ï¼Œåˆ™è¿”å›çš„æ˜¯å¤šä¸ªæ ·æœ¬è¢«é¢„å¤„ç†ä¹‹åçš„ç»“æœlistã€‚\n",
    "\n",
    "æ¥ä¸‹æ¥**ä½¿ç”¨mapå‡½æ•°**å¯¹æ•°æ®é›†**datasetsé‡Œé¢ä¸‰ä¸ªæ ·æœ¬é›†åˆçš„æ‰€æœ‰æ ·æœ¬è¿›è¡Œé¢„å¤„ç†ï¼Œ**å°†é¢„å¤„ç†å‡½æ•°`preprocess_function`åº”ç”¨åˆ°ï¼ˆmap)æ‰€æœ‰æ ·æœ¬ä¸Šã€‚å‚æ•°`batched=True`å¯ä»¥æ‰¹é‡å¯¹æ–‡æœ¬è¿›è¡Œç¼–ç ã€‚è¿™æ˜¯ä¸ºäº†å……åˆ†åˆ©ç”¨å‰é¢åŠ è½½fast_tokenizerçš„ä¼˜åŠ¿ï¼Œå®ƒå°†ä½¿ç”¨å¤šçº¿ç¨‹å¹¶å‘åœ°å¤„ç†æ‰¹ä¸­çš„æ–‡æœ¬ã€‚"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "tokenized_datasets = raw_datasets.map(preprocess_function, batched=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## å¾®è°ƒé¢„è®­ç»ƒæ¨¡å‹\n",
    "\n",
    "æ•°æ®å·²ç»å‡†å¤‡å¥½äº†ï¼Œæˆ‘ä»¬éœ€è¦ä¸‹è½½å¹¶åŠ è½½é¢„è®­ç»ƒæ¨¡å‹ï¼Œç„¶åå¾®è°ƒé¢„è®­ç»ƒæ¨¡å‹ã€‚\n",
    "\n",
    "### åŠ è½½é¢„è®­ç»ƒæ¨¡å‹\n",
    "\n",
    "åš**seq2seqä»»åŠ¡ï¼Œé‚£ä¹ˆéœ€è¦ä¸€ä¸ªèƒ½è§£å†³è¿™ä¸ªä»»åŠ¡çš„æ¨¡å‹ç±»ã€‚æˆ‘ä»¬ä½¿ç”¨`AutoModelForSeq2SeqLM` è¿™ä¸ªç±»**ã€‚\n",
    "\n",
    "å’Œä¹‹å‰å‡ ç¯‡åšå®¢æåˆ°çš„åŠ è½½æ–¹å¼ç›¸åŒä¸å†èµ˜è¿°ã€‚"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import AutoModelForSeq2SeqLM, \n",
    "model = AutoModelForSeq2SeqLM.from_pretrained(model_checkpoint)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### è®¾å®šè®­ç»ƒå‚æ•°\n",
    "\n",
    "ä¸ºäº†èƒ½å¤Ÿå¾—åˆ°ä¸€ä¸ª**`Seq2SeqTrainer`è®­ç»ƒå·¥å…·**ï¼Œæˆ‘ä»¬è¿˜éœ€è¦**è®­ç»ƒçš„è®¾å®š/å‚æ•° [`Seq2SeqTrainingArguments`](https://huggingface.co/transformers/main_classes/trainer.html#transformers.Seq2SeqTrainingArguments)ã€‚è¿™ä¸ªè®­ç»ƒè®¾å®šåŒ…å«äº†èƒ½å¤Ÿå®šä¹‰è®­ç»ƒè¿‡ç¨‹çš„æ‰€æœ‰å±æ€§**ã€‚\n",
    "\n",
    "ç”±äºæ•°æ®é›†æ¯”è¾ƒå¤§ï¼Œ`Seq2SeqTrainer`è®­ç»ƒæ—¶ä¼šåŒæ—¶ä¸æ–­ä¿å­˜æ¨¡å‹ï¼Œæˆ‘ä»¬ç”¨`save_total_limit=3`å‚æ•°æ§åˆ¶è‡³å¤šä¿å­˜3ä¸ªæ¨¡å‹ã€‚"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import Seq2SeqTrainingArguments\n",
    "batch_size = 16\n",
    "args = Seq2SeqTrainingArguments(\n",
    "    \"test-translation\",\n",
    "    evaluation_strategy = \"epoch\",\n",
    "    learning_rate=2e-5,\n",
    "    per_device_train_batch_size=batch_size,\n",
    "    per_device_eval_batch_size=batch_size,\n",
    "    weight_decay=0.01,\n",
    "    save_total_limit=3, #è‡³å¤šä¿å­˜æ¨¡å‹ä¸ªæ•°\n",
    "    num_train_epochs=1,\n",
    "    predict_with_generate=True,\n",
    "    fp16=False,\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### æ•°æ®æ”¶é›†å™¨data collator\n",
    "\n",
    "æ¥ä¸‹æ¥éœ€è¦å‘Šè¯‰`Trainer`å¦‚ä½•ä»é¢„å¤„ç†çš„è¾“å…¥æ•°æ®ä¸­æ„é€ batchã€‚æˆ‘ä»¬ä½¿ç”¨æ•°æ®æ”¶é›†å™¨`DataCollatorForSeq2Seq`ï¼Œå°†ç»é¢„å¤„ç†çš„è¾“å…¥åˆ†batchå†æ¬¡å¤„ç†åå–‚ç»™æ¨¡å‹ã€‚"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import DataCollatorForSeq2Seq\n",
    "data_collator = DataCollatorForSeq2Seq(tokenizer, model=model)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### å®šä¹‰è¯„ä¼°æ–¹æ³•\n",
    "\n",
    "æˆ‘ä»¬ä½¿ç”¨`'bleu'`æŒ‡æ ‡ï¼Œåˆ©ç”¨`metric.compute`è®¡ç®—è¯¥æŒ‡æ ‡å¯¹æ¨¡å‹è¿›è¡Œè¯„ä¼°ã€‚\n",
    "\n",
    "`metric.compute`å¯¹æ¯”predictionså’Œlabelsï¼Œä»è€Œè®¡ç®—å¾—åˆ†ã€‚predictionså’Œlabelséƒ½éœ€è¦æ˜¯ä¸€ä¸ªlistã€‚å…·ä½“æ ¼å¼è§ä¸‹é¢çš„ä¾‹å­ï¼š"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fake_preds = [\"hello there\", \"general kenobi\"]\n",
    "fake_labels = [[\"hello there\"], [\"general kenobi\"]]\n",
    "metric.compute(predictions=fake_preds, references=fake_labels)\n",
    "#{'bp': 1.0,\n",
    "# 'counts': [4, 2, 0, 0],\n",
    "# 'precisions': [100.0, 100.0, 0.0, 0.0],\n",
    "# 'ref_len': 4,\n",
    "# 'score': 0.0,\n",
    "# 'sys_len': 4,\n",
    "# 'totals': [4, 2, 0, 0]}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "å°†æ¨¡å‹é¢„æµ‹é€å…¥è¯„ä¼°ä¹‹å‰ï¼Œè¿˜éœ€è¦å†™`postprocess_text`å‡½æ•°åšä¸€äº›æ•°æ®åå¤„ç†ï¼š"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "from datasets import load_metric\n",
    "metric = load_metric(\"sacrebleu\")\n",
    "\n",
    "def postprocess_text(preds, labels):\n",
    "    preds = [pred.strip() for pred in preds]\n",
    "    labels = [[label.strip()] for label in labels]\n",
    "\n",
    "    return preds, labels\n",
    "\n",
    "def compute_metrics(eval_preds):\n",
    "    preds, labels = eval_preds\n",
    "    if isinstance(preds, tuple):\n",
    "        preds = preds[0]\n",
    "    decoded_preds = tokenizer.batch_decode(preds, skip_special_tokens=True)\n",
    "\n",
    "    # Replace -100 in the labels as we can't decode them.\n",
    "    labels = np.where(labels != -100, labels, tokenizer.pad_token_id)\n",
    "    decoded_labels = tokenizer.batch_decode(labels, skip_special_tokens=True)\n",
    "\n",
    "    # Some simple post-processing\n",
    "    decoded_preds, decoded_labels = postprocess_text(decoded_preds, decoded_labels)\n",
    "\n",
    "    result = metric.compute(predictions=decoded_preds, references=decoded_labels)\n",
    "    result = {\"bleu\": result[\"score\"]}\n",
    "\n",
    "    prediction_lens = [np.count_nonzero(pred != tokenizer.pad_token_id) for pred in preds]\n",
    "    result[\"gen_len\"] = np.mean(prediction_lens)\n",
    "    result = {k: round(v, 4) for k, v in result.items()}\n",
    "    return result"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### å¼€å§‹è®­ç»ƒ\n",
    "\n",
    "å°†æ•°æ®/æ¨¡å‹/å‚æ•°ä¼ å…¥`Trainer`å³å¯ï¼š"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import Seq2SeqTrainer\n",
    "trainer = Seq2SeqTrainer(\n",
    "    model,\n",
    "    args,\n",
    "    train_dataset=tokenized_datasets[\"train\"],\n",
    "    eval_dataset=tokenized_datasets[\"validation\"],\n",
    "    data_collator=data_collator,\n",
    "    tokenizer=tokenizer,\n",
    "    compute_metrics=compute_metrics\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "è°ƒç”¨`train`æ–¹æ³•å¼€å§‹è®­ç»ƒï¼š"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "trainer.train()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## å‚è€ƒæ–‡çŒ®\n",
    "\n",
    "[4.6-ç”Ÿæˆä»»åŠ¡-æœºå™¨ç¿»è¯‘.md](https://github.com/datawhalechina/learn-nlp-with-transformers/blob/main/docs/ç¯‡ç« 4-ä½¿ç”¨Transformersè§£å†³NLPä»»åŠ¡/4.6-ç”Ÿæˆä»»åŠ¡-æœºå™¨ç¿»è¯‘.md)\n",
    "\n",
    "[BERTç›¸å…³â€”â€”ï¼ˆ7ï¼‰å°†BERTåº”ç”¨åˆ°ä¸‹æ¸¸ä»»åŠ¡](https://ifwind.github.io/2021/08/24/BERTç›¸å…³â€”â€”ï¼ˆ7ï¼‰æŠŠBERTåº”ç”¨åˆ°ä¸‹æ¸¸ä»»åŠ¡/)\n",
    "\n",
    "[transformerså®˜æ–¹æ–‡æ¡£](https://huggingface.co/transformers)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
