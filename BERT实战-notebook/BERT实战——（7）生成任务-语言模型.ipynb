{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "# BERTå®æˆ˜â€”â€”ï¼ˆ7ï¼‰ç”Ÿæˆä»»åŠ¡-è¯­è¨€æ¨¡å‹\n",
    "\n",
    "## å¼•è¨€\n",
    "\n",
    "ä¹‹å‰çš„åˆ†åˆ«ä»‹ç»äº†ä½¿ç”¨ [ğŸ¤— Transformers](https://github.com/huggingface/transformers)ä»£ç åº“ä¸­çš„æ¨¡å‹å¼€å±•one-classä»»åŠ¡([æ–‡æœ¬åˆ†ç±»](https://ifwind.github.io/2021/08/26/BERT%E5%AE%9E%E6%88%98%E2%80%94%E2%80%94%EF%BC%881%EF%BC%89%E6%96%87%E6%9C%AC%E5%88%86%E7%B1%BB/)ã€[å¤šé€‰é—®ç­”é—®é¢˜](https://ifwind.github.io/2021/08/27/BERT%E5%AE%9E%E6%88%98%E2%80%94%E2%80%94%EF%BC%883%EF%BC%89%E9%97%AE%E7%AD%94%E4%BB%BB%E5%8A%A1-%E5%A4%9A%E9%80%89%E9%97%AE%E7%AD%94/))ã€class for each tokenä»»åŠ¡([åºåˆ—æ ‡æ³¨](https://ifwind.github.io/2021/08/27/BERT%E5%AE%9E%E6%88%98%E2%80%94%E2%80%94%EF%BC%882%EF%BC%89%E5%BA%8F%E5%88%97%E6%A0%87%E6%B3%A8/))ã€copy from inputä»»åŠ¡([æŠ½å–å¼é—®ç­”](https://ifwind.github.io/2021/08/30/BERT%E5%AE%9E%E6%88%98%E2%80%94%E2%80%94%EF%BC%884%EF%BC%89%E9%97%AE%E7%AD%94%E4%BB%BB%E5%8A%A1-%E6%8A%BD%E5%8F%96%E5%BC%8F%E9%97%AE%E7%AD%94/))ä»¥åŠgeneral sequenceä»»åŠ¡ï¼ˆæœºå™¨ç¿»è¯‘ã€æ‘˜è¦æŠ½å–ï¼‰ã€‚\n",
    "\n",
    "è¿™ä¸€ç¯‡å°†ä»‹ç»**å¦‚ä½•ä½¿ç”¨è¯­è¨€æ¨¡å‹ä»»åŠ¡å¾®è°ƒ [ğŸ¤— Transformers](https://github.com/huggingface/transformers)æ¨¡å‹**ï¼ˆå…³äºä»€ä¹ˆæ˜¯è¯­è¨€æ¨¡å‹ï¼Œå›çœ‹[ä¹‹å‰çš„åšå®¢-è¯­è¨€æ¨¡å‹](https://ifwind.github.io/2021/08/20/BERT%E7%9B%B8%E5%85%B3%E2%80%94%E2%80%94%EF%BC%881%EF%BC%89%E8%AF%AD%E8%A8%80%E6%A8%A1%E5%9E%8B/)ï¼‰ã€‚\n",
    "\n",
    "### ä»»åŠ¡ä»‹ç»\n",
    "\n",
    "æˆ‘ä»¬è¿™é‡Œä¸»è¦å®Œæˆä¸¤ç±»è¯­è¨€å»ºæ¨¡ä»»åŠ¡ï¼š\n",
    "\n",
    "+ **å› æœè¯­è¨€æ¨¡å‹ï¼ˆCausal language modelingï¼ŒCLMï¼‰**ï¼š**æ¨¡å‹éœ€è¦é¢„æµ‹å¥å­ä¸­çš„ä¸‹ä¸€ä½ç½®å¤„çš„å­—ç¬¦**ï¼ˆ**ç±»ä¼¼BERTç±»æ¨¡å‹çš„decoderå’ŒGPT**ï¼Œä»å·¦å¾€å³è¾“å…¥å­—ç¬¦ï¼‰ã€‚æ¨¡å‹ä¼šä½¿ç”¨çŸ©é˜µå¯¹è§’çº¿attention maskæœºåˆ¶é˜²æ­¢æ¨¡å‹æå‰çœ‹åˆ°ç­”æ¡ˆã€‚ä¾‹å¦‚ï¼Œå½“æ¨¡å‹è¯•å›¾é¢„æµ‹å¥å­ä¸­çš„$i+1$ä½ç½®å¤„çš„å­—ç¬¦æ—¶ï¼Œè¿™ä¸ªæ©ç å°†é˜»æ­¢å®ƒè®¿é—®$i$ä½ç½®ä¹‹åçš„å­—ç¬¦ã€‚\n",
    "\n",
    "> <img src=\"BERTå®æˆ˜â€”â€”ï¼ˆ7ï¼‰ç”Ÿæˆä»»åŠ¡-è¯­è¨€æ¨¡å‹\\image-20210901190347576.png\" style=\"zoom:80%;\" />\n",
    "\n",
    "+ **æ©ç è¯­è¨€å»ºæ¨¡ï¼ˆMasked language modelingï¼ŒMLMï¼‰**ï¼šæ¨¡å‹éœ€è¦æ¢å¤è¾“å…¥ä¸­è¢«\"MASK\"æ‰çš„ä¸€äº›å­—ç¬¦ï¼ˆBERTç±»æ¨¡å‹çš„é¢„è®­ç»ƒä»»åŠ¡ï¼Œåªç”¨transformerçš„encoderéƒ¨åˆ†ï¼‰ã€‚æ¨¡å‹å¯ä»¥çœ‹åˆ°æ•´ä¸ªå¥å­ï¼Œå› æ­¤æ¨¡å‹å¯ä»¥æ ¹æ®â€œ\\[MASK\\]â€æ ‡è®°ä¹‹å‰å’Œä¹‹åçš„å­—ç¬¦æ¥é¢„æµ‹è¯¥ä½ç½®è¢«â€œ\\[MASK\\]â€ä¹‹å‰çš„å­—ç¬¦ã€‚\n",
    "\n",
    "> <img src=\"BERTå®æˆ˜â€”â€”ï¼ˆ7ï¼‰ç”Ÿæˆä»»åŠ¡-è¯­è¨€æ¨¡å‹\\masked_language_modeling.png\" style=\"zoom:80%;\" />\n",
    "\n",
    "ä¸»è¦åˆ†ä¸ºä»¥ä¸‹å‡ ä¸ªéƒ¨åˆ†ï¼š\n",
    "\n",
    "1. æ•°æ®åŠ è½½ï¼›\n",
    "2. æ•°æ®é¢„å¤„ç†ï¼›\n",
    "3. å¾®è°ƒé¢„è®­ç»ƒæ¨¡å‹ï¼šä½¿ç”¨transformerä¸­çš„**`Seq2SeqTrainer`æ¥å£**å¯¹é¢„è®­ç»ƒæ¨¡å‹è¿›è¡Œå¾®è°ƒï¼ˆæ³¨æ„è¿™é‡Œæ˜¯`Seq2SeqTrainer`æ¥å£ï¼Œä¹‹å‰çš„ä»»åŠ¡éƒ½æ˜¯è°ƒç”¨`Trainer`æ¥å£ï¼‰ã€‚\n",
    "\n",
    "### å‰æœŸå‡†å¤‡\n",
    "\n",
    "å®‰è£…ä»¥ä¸‹åº“ï¼š"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pip install datasets transformers sacrebleu sentencepiece\n",
    "#transformers==4.9.2\n",
    "#datasets==1.11.0"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## æ•°æ®åŠ è½½\n",
    "\n",
    "### æ•°æ®é›†ä»‹ç»\n",
    "\n",
    "æˆ‘ä»¬ä½¿ç”¨[Wikitext 2](https://huggingface.co/datasets/wikitext#data-instances)æ•°æ®é›†ï¼Œå…¶ä¸­åŒ…æ‹¬äº†ä»Wikipediaä¸Šç»è¿‡éªŒè¯çš„Goodå’ŒFeaturedæ–‡ç« é›†ä¸­æå–çš„è¶…è¿‡1äº¿ä¸ªtokençš„é›†åˆã€‚\n",
    "\n",
    "### åŠ è½½æ•°æ®\n",
    "\n",
    "è¯¥æ•°æ®çš„åŠ è½½æ–¹å¼åœ¨transformersåº“ä¸­è¿›è¡Œäº†å°è£…ï¼Œæˆ‘ä»¬å¯ä»¥é€šè¿‡ä»¥ä¸‹è¯­å¥è¿›è¡Œæ•°æ®åŠ è½½ï¼š"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from datasets import load_dataset\n",
    "datasets = load_dataset('wikitext', 'wikitext-2-raw-v1')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "> å¦‚æœç¢°åˆ°ä»¥ä¸‹é”™è¯¯ï¼š\n",
    "> ![request Error](BERTå®æˆ˜â€”â€”ï¼ˆ7ï¼‰ç”Ÿæˆä»»åŠ¡-è¯­è¨€æ¨¡å‹\\request_error.png)\n",
    ">\n",
    "> è§£å†³æ–¹æ¡ˆ:\n",
    ">\n",
    "> MACç”¨æˆ·: åœ¨ ```/etc/hosts``` æ–‡ä»¶ä¸­æ·»åŠ ä¸€è¡Œ ```199.232.68.133  raw.githubusercontent.com```\n",
    ">\n",
    "> Windowsç”¨æˆ·: åœ¨ ```C:\\Windows\\System32\\drivers\\etc\\hosts```  æ–‡ä»¶ä¸­æ·»åŠ ä¸€è¡Œ ```199.232.68.133  raw.githubusercontent.com```\n",
    "\n",
    "å¦‚æœæƒ³åŠ è½½è‡ªå·±çš„æ•°æ®é›†å¯ä»¥å‚è€ƒ[ä¹‹å‰çš„åšå®¢-å®šä½è¯ï¼šåŠ è½½è‡ªå·±çš„æ•°æ®æˆ–æ¥è‡ªç½‘ç»œçš„æ•°æ®](https://ifwind.github.io/2021/08/26/BERTå®æˆ˜â€”â€”ï¼ˆ1ï¼‰æ–‡æœ¬åˆ†ç±»/#åŠ è½½è‡ªå·±çš„æ•°æ®æˆ–æ¥è‡ªç½‘ç»œçš„æ•°æ®)ã€‚\n",
    "\n",
    "æ•°æ®åŠ è½½å®Œæ¯•åï¼Œç»™å®šä¸€ä¸ªæ•°æ®åˆ‡åˆ†çš„keyï¼ˆtrainã€validationæˆ–è€…testï¼‰å’Œä¸‹æ ‡å³å¯æŸ¥çœ‹æ•°æ®ã€‚"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "datasets[\"train\"][10]\n",
    "#{'text': ' The game \\'s battle system , the BliTZ system , is carried over directly from Valkyira Chronicles . During missions , players select each unit using a top @-@ down perspective of the battlefield map : once a character is selected , the player moves the character around the battlefield in third @-@ person . A character can only act once per @-@ turn , but characters can be granted multiple turns at the expense of other characters \\' turns . Each character has a field and distance of movement limited by their Action Gauge . Up to nine characters can be assigned to a single mission . During gameplay , characters will call out if something happens to them , such as their health points ( HP ) getting low or being knocked out by enemy attacks . Each character has specific \" Potentials \" , skills unique to each character . They are divided into \" Personal Potential \" , which are innate skills that remain unaltered unless otherwise dictated by the story and can either help or impede a character , and \" Battle Potentials \" , which are grown throughout the game and always grant boons to a character . To learn Battle Potentials , each character has a unique \" Masters Table \" , a grid @-@ based skill table that can be used to acquire and link different skills . Characters also have Special Abilities that grant them temporary boosts on the battlefield : Kurt can activate \" Direct Command \" and move around the battlefield without depleting his Action Point gauge , the character Reila can shift into her \" Valkyria Form \" and become invincible , while Imca can target multiple enemy units with her heavy weapon . \\n'}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "ä¸‹é¢çš„å‡½æ•°å°†ä»æ•°æ®é›†é‡Œéšæœºé€‰æ‹©å‡ ä¸ªä¾‹å­è¿›è¡Œå±•ç¤ºï¼Œå¯ä»¥çœ‹åˆ°ä¸€äº›æ–‡æœ¬æ˜¯ç»´åŸºç™¾ç§‘æ–‡ç« çš„å®Œæ•´æ®µè½ï¼Œè€Œå…¶ä»–çš„åªæ˜¯æ ‡é¢˜æˆ–ç©ºè¡Œã€‚"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from datasets import ClassLabel\n",
    "import random\n",
    "import pandas as pd\n",
    "from IPython.display import display, HTML\n",
    "\n",
    "def show_random_elements(dataset, num_examples=4):\n",
    "    assert num_examples <= len(dataset), \"Can't pick more elements than there are in the dataset.\"\n",
    "    picks = []\n",
    "    for _ in range(num_examples):\n",
    "        pick = random.randint(0, len(dataset)-1)\n",
    "        while pick in picks:\n",
    "            pick = random.randint(0, len(dataset)-1)\n",
    "        picks.append(pick)\n",
    "    \n",
    "    df = pd.DataFrame(dataset[picks])\n",
    "    for column, typ in dataset.features.items():\n",
    "        if isinstance(typ, ClassLabel):\n",
    "            df[column] = df[column].transform(lambda i: typ.names[i])\n",
    "    display(HTML(df.to_html()))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "show_random_elements(raw_datasets[\"train\"])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<table border=\"1\" class=\"dataframe\">\n",
    "  <thead>\n",
    "    <tr style=\"text-align: right;\">\n",
    "      <th></th>\n",
    "      <th>text</th>\n",
    "    </tr>\n",
    "  </thead>\n",
    "  <tbody>\n",
    "    <tr>\n",
    "      <th>0</th>\n",
    "      <td>MD 194D is the designation for an unnamed 0 @.@ 02 @-@ mile ( 0 @.@ 032 km ) connector between MD 194 and MD 853E , the old alignment that parallels the northbound direction of the modern highway south of Angell Road . \\n</td>\n",
    "    </tr>\n",
    "    <tr>\n",
    "      <th>1</th>\n",
    "      <td>My sense , as though of hemlock I had drunk , \\n</td>\n",
    "    </tr>\n",
    "    <tr>\n",
    "      <th>2</th>\n",
    "      <td></td>\n",
    "    </tr>\n",
    "    <tr>\n",
    "      <th>3</th>\n",
    "      <td>A mimed stage show , Thunderbirds : F.A.B. , has toured internationally and popularised a staccato style of movement known colloquially as the \" Thunderbirds walk \" . The production has periodically been revived as Thunderbirds : F.A.B. â€“ The Next Generation . \\n</td>\n",
    "    </tr>\n",
    "  </tbody>\n",
    "</table>\n",
    "\n",
    "## å› æœè¯­è¨€æ¨¡å‹ï¼ˆCausal Language Modelingï¼ŒCLMï¼‰\n",
    "\n",
    "### æ•°æ®é¢„å¤„ç†\n",
    "\n",
    "åœ¨å°†æ•°æ®å–‚å…¥æ¨¡å‹ä¹‹å‰ï¼Œæˆ‘ä»¬éœ€è¦å¯¹æ•°æ®è¿›è¡Œé¢„å¤„ç†ã€‚\n",
    "\n",
    "ä»ç„¶æ˜¯ä¸¤ä¸ªæ•°æ®é¢„å¤„ç†çš„åŸºæœ¬æµç¨‹ï¼š\n",
    "\n",
    "1. åˆ†è¯ï¼›\n",
    "2. è½¬åŒ–æˆå¯¹åº”ä»»åŠ¡è¾“å…¥æ¨¡å‹çš„æ ¼å¼ï¼›\n",
    "\n",
    "`Tokenizer`ç”¨äºä¸Šé¢ä¸¤æ­¥æ•°æ®é¢„å¤„ç†å·¥ä½œï¼š`Tokenizer`é¦–å…ˆå¯¹è¾“å…¥è¿›è¡Œtokenizeï¼Œç„¶åå°†tokensè½¬åŒ–ä¸ºé¢„æ¨¡å‹ä¸­éœ€è¦å¯¹åº”çš„token IDï¼Œå†è½¬åŒ–ä¸ºæ¨¡å‹éœ€è¦çš„è¾“å…¥æ ¼å¼ã€‚\n",
    "\n",
    "#### åˆå§‹åŒ–Tokenizer\n",
    "\n",
    "[ä¹‹å‰çš„åšå®¢](https://ifwind.github.io/2021/08/26/BERT%E5%AE%9E%E6%88%98%E2%80%94%E2%80%94%EF%BC%881%EF%BC%89%E6%96%87%E6%9C%AC%E5%88%86%E7%B1%BB/#%E5%88%9D%E5%A7%8B%E5%8C%96Tokenizer)å·²ç»ä»‹ç»äº†ä¸€äº›Tokenizerçš„å†…å®¹ï¼Œå¹¶åšäº†Tokenizeråˆ†è¯çš„ç¤ºä¾‹ï¼Œè¿™é‡Œä¸å†é‡å¤ã€‚`use_fast=True`æŒ‡å®šä½¿ç”¨fastç‰ˆæœ¬çš„tokenizerã€‚æˆ‘ä»¬ä½¿ç”¨å·²ç»è®­ç»ƒå¥½çš„[`distilgpt2`](https://huggingface.co/distilgpt2) æ¨¡å‹checkpointæ¥åšè¯¥ä»»åŠ¡ã€‚"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import AutoTokenizer\n",
    "model_checkpoint = \"distilgpt2\"\n",
    "tokenizer = AutoTokenizer.from_pretrained(model_checkpoint, use_fast=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### è½¬åŒ–æˆå¯¹åº”ä»»åŠ¡è¾“å…¥æ¨¡å‹çš„æ ¼å¼\n",
    "\n",
    "å¯¹äºå› æœè¯­è¨€æ¨¡å‹(CLM)ï¼Œæˆ‘ä»¬é¦–å…ˆè·å–åˆ°æ•°æ®é›†ä¸­çš„æ‰€æœ‰æ–‡æœ¬å¹¶åˆ†è¯ï¼Œä¹‹åå°†å®ƒä»¬è¿æ¥èµ·æ¥ã€‚æœ€åï¼Œåœ¨ç‰¹å®šåºåˆ—é•¿åº¦çš„ä¾‹å­ä¸­æ‹†åˆ†å®ƒä»¬ï¼Œå°†å„ä¸ªæ‹†åˆ†éƒ¨åˆ†ä½œä¸ºæ¨¡å‹è¾“å…¥ã€‚\n",
    "\n",
    "é€šè¿‡è¿™ç§æ–¹å¼ï¼Œæ¨¡å‹å°†æ¥æ”¶å¦‚ä¸‹çš„è¿ç»­æ–‡æœ¬å—ï¼Œ`[BOS_TOKEN]`ç”¨äºåˆ†å‰²æ‹¼æ¥äº†æ¥è‡ªä¸åŒå†…å®¹çš„æ–‡æœ¬ã€‚"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "è¾“å…¥ç±»å‹1ï¼šæ–‡æœ¬1\n",
    "è¾“å…¥ç±»å‹2ï¼šæ–‡æœ¬1ç»“å°¾ [BOS_TOKEN] æ–‡æœ¬2å¼€å¤´"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### è°ƒç”¨åˆ†è¯å™¨å¯¹æ‰€æœ‰çš„æ–‡æœ¬åˆ†è¯"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def tokenize_function(examples):\n",
    "    return tokenizer(examples[\"text\"])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**ä½¿ç”¨mapå‡½æ•°**å¯¹æ•°æ®é›†**datasetsé‡Œé¢ä¸‰ä¸ªæ ·æœ¬é›†åˆçš„æ‰€æœ‰æ ·æœ¬è¿›è¡Œé¢„å¤„ç†ï¼Œ**å°†å‡½æ•°`tokenize_function`åº”ç”¨åˆ°ï¼ˆmap)æ‰€æœ‰æ ·æœ¬ä¸Šã€‚ä½¿ç”¨```batch=True```å’Œ```4```ä¸ªè¿›ç¨‹æ¥åŠ é€Ÿé¢„å¤„ç†ã€‚è¿™æ˜¯ä¸ºäº†å……åˆ†åˆ©ç”¨å‰é¢åŠ è½½fast_tokenizerçš„ä¼˜åŠ¿ï¼Œå®ƒå°†ä½¿ç”¨å¤šçº¿ç¨‹å¹¶å‘åœ°å¤„ç†æ‰¹ä¸­çš„æ–‡æœ¬ã€‚ä¹‹åæˆ‘ä»¬å¹¶ä¸éœ€è¦```text```åˆ—ï¼Œæ‰€ä»¥å°†å…¶èˆå¼ƒï¼ˆ`remove_columns`ï¼‰ã€‚"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "tokenized_datasets = datasets.map(tokenize_function, batched=True, num_proc=4, remove_columns=[\"text\"])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "æˆ‘ä»¬ç°åœ¨æŸ¥çœ‹æ•°æ®é›†çš„ä¸€ä¸ªå…ƒç´ ï¼Œè®­ç»ƒé›†ä¸­`text`å·²ç»è¢«æ¨¡å‹æ‰€éœ€çš„`input_ids`æ‰€å–ä»£:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "tokenized_datasets[\"train\"][1]\n",
    "#{'attention_mask': [1, 1, 1, 1, 1, 1, 1, 1, 1],\n",
    "# 'input_ids': [796, 569, 18354, 7496, 17740, 6711, 796, 220, 198]}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### æ‹¼æ¥æ–‡æœ¬&æŒ‰å®šé•¿æ‹†åˆ†æ–‡æœ¬\n",
    "\n",
    "æˆ‘ä»¬éœ€è¦å°†æ‰€æœ‰æ–‡æœ¬è¿æ¥åœ¨ä¸€èµ·ï¼Œç„¶åå°†ç»“æœåˆ†å‰²æˆç‰¹å®š`block_size`çš„å°å—ã€‚`block_size`è®¾ç½®ä¸ºé¢„è®­ç»ƒæ¨¡å‹æ—¶æ‰€ä½¿ç”¨çš„æœ€å¤§é•¿åº¦ã€‚\n",
    "\n",
    "ç¼–å†™é¢„å¤„ç†å‡½æ•°æ¥å¯¹æ–‡æœ¬è¿›è¡Œç»„åˆå’Œæ‹†åˆ†ï¼š"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# block_size = tokenizer.model_max_length\n",
    "block_size = 128\n",
    "def group_texts(examples):\n",
    "    # æ‹¼æ¥æ‰€æœ‰æ–‡æœ¬\n",
    "    concatenated_examples = {k: sum(examples[k], []) for k in examples.keys()}\n",
    "    total_length = len(concatenated_examples[list(examples.keys())[0]])\n",
    "    # æˆ‘ä»¬å°†ä½™æ•°å¯¹åº”çš„éƒ¨åˆ†å»æ‰ã€‚ä½†å¦‚æœæ¨¡å‹æ”¯æŒçš„è¯ï¼Œå¯ä»¥æ·»åŠ paddingï¼Œæ‚¨å¯ä»¥æ ¹æ®éœ€è¦å®šåˆ¶æ­¤éƒ¨ä»¶ã€‚\n",
    "    total_length = (total_length // block_size) * block_size\n",
    "    # é€šè¿‡max_lenè¿›è¡Œåˆ†å‰²ã€‚\n",
    "    result = {\n",
    "        k: [t[i : i + block_size] for i in range(0, total_length, block_size)]\n",
    "        for k, t in concatenated_examples.items()\n",
    "    }\n",
    "    result[\"labels\"] = result[\"input_ids\"].copy()\n",
    "    return result"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**æ³¨æ„ï¼šå› ä¸ºæˆ‘ä»¬åšçš„æ˜¯å› æœè¯­è¨€æ¨¡å‹ï¼Œå…¶é¢„æµ‹labelå°±æ˜¯å…¶è¾“å…¥çš„input_idï¼Œæ‰€ä»¥æˆ‘ä»¬å¤åˆ¶äº†æ ‡ç­¾çš„è¾“å…¥ã€‚**\n",
    "\n",
    "æˆ‘ä»¬å°†å†æ¬¡ä½¿ç”¨`map`æ–¹æ³•ï¼Œ`batched=True`è¡¨ç¤ºå…è®¸é€šè¿‡è¿”å›ä¸åŒæ•°é‡çš„æ ·æœ¬æ¥æ”¹å˜æ•°æ®é›†ä¸­çš„æ ·æœ¬æ•°é‡ï¼Œè¿™æ ·å¯ä»¥ä»ä¸€æ‰¹ç¤ºä¾‹ä¸­åˆ›å»ºæ–°çš„ç¤ºä¾‹ã€‚\n",
    "\n",
    "æ³¨æ„ï¼Œåœ¨é»˜è®¤æƒ…å†µä¸‹ï¼Œ`map`æ–¹æ³•å°†å‘é€ä¸€æ‰¹1,000ä¸ªç¤ºä¾‹ï¼Œç”±é¢„å¤„ç†å‡½æ•°å¤„ç†ã€‚**å¯ä»¥é€šè¿‡ä¼ é€’ä¸åŒbatch_sizeæ¥è°ƒæ•´ã€‚ä¹Ÿå¯ä»¥ä½¿ç”¨`num_proc `æ¥åŠ é€Ÿé¢„å¤„ç†ã€‚**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "lm_datasets = tokenized_datasets.map(\n",
    "    group_texts,\n",
    "    batched=True,\n",
    "    batch_size=1000,\n",
    "    num_proc=4,\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "æ£€æŸ¥ä¸€ä¸‹æ•°æ®é›†æ˜¯å¦å‘ç”Ÿäº†å˜åŒ–ï¼š"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "tokenizer.decode(lm_datasets[\"train\"][1][\"input_ids\"])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "ç°åœ¨æ ·æœ¬åŒ…å«äº†`block_size`å¤§å°çš„è¿ç»­å­—ç¬¦å—ï¼Œå¯èƒ½è·¨è¶Šäº†å‡ ä¸ªåŸå§‹æ–‡æœ¬ã€‚"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "' game and follows the \" Nameless \", a penal military unit serving the nation of Gallia during the Second Europan War who perform secret black operations and are pitted against the Imperial unit \" Calamaty Raven \". \\n The game began development in 2010, carrying over a large portion of the work done on Valkyria Chronicles II. While it retained the standard features of the series, it also underwent multiple adjustments, such as making the game more forgiving for series newcomers. Character designer Raita Honjou and composer Hitoshi Sakimoto both returned from previous entries, along with Valkyria Chronicles II director Takeshi Oz'"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### å¾®è°ƒé¢„è®­ç»ƒæ¨¡å‹\n",
    "\n",
    "æ•°æ®å·²ç»å‡†å¤‡å¥½äº†ï¼Œæˆ‘ä»¬éœ€è¦ä¸‹è½½å¹¶åŠ è½½é¢„è®­ç»ƒæ¨¡å‹ï¼Œç„¶åå¾®è°ƒé¢„è®­ç»ƒæ¨¡å‹ã€‚\n",
    "\n",
    "#### åŠ è½½é¢„è®­ç»ƒæ¨¡å‹\n",
    "\n",
    "åš**å› æœè¯­è¨€æ¨¡å‹ä»»åŠ¡ï¼Œé‚£ä¹ˆéœ€è¦ä¸€ä¸ªèƒ½è§£å†³è¿™ä¸ªä»»åŠ¡çš„æ¨¡å‹ç±»ã€‚æˆ‘ä»¬ä½¿ç”¨`AutoModelForCausalLM` è¿™ä¸ªç±»**ã€‚\n",
    "\n",
    "å’Œä¹‹å‰å‡ ç¯‡åšå®¢æåˆ°çš„åŠ è½½æ–¹å¼ç›¸åŒä¸å†èµ˜è¿°ã€‚"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import AutoModelForCausalLM\n",
    "model = AutoModelForCausalLM.from_pretrained(model_checkpoint)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### è®¾å®šè®­ç»ƒå‚æ•°\n",
    "\n",
    "ä¸ºäº†èƒ½å¤Ÿå¾—åˆ°ä¸€ä¸ª**`Trainer`è®­ç»ƒå·¥å…·**ï¼Œæˆ‘ä»¬è¿˜éœ€è¦**è®­ç»ƒçš„è®¾å®š/å‚æ•° [`TrainingArguments`](https://huggingface.co/transformers/main_classes/trainer.html#transformers.TrainingArguments)ã€‚è¿™ä¸ªè®­ç»ƒè®¾å®šåŒ…å«äº†èƒ½å¤Ÿå®šä¹‰è®­ç»ƒè¿‡ç¨‹çš„æ‰€æœ‰å±æ€§**ã€‚"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import TrainingArguments\n",
    "batch_size = 16\n",
    "training_args = TrainingArguments(\n",
    "    \"test-clm\",\n",
    "    evaluation_strategy = \"epoch\",\n",
    "    learning_rate=2e-5,\n",
    "    weight_decay=0.01,\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### å®šä¹‰è¯„ä¼°æ–¹æ³•\n",
    "\n",
    "å®Œæˆè¯¥ä»»åŠ¡ä¸éœ€è¦ç‰¹åˆ«å®šä¹‰è¯„ä¼°æŒ‡æ ‡å¤„ç†å‡½æ•°ï¼Œæ¨¡å‹å°†ç›´æ¥è®¡ç®—å›°æƒ‘åº¦perplexityä½œä¸ºè¯„ä¼°æŒ‡æ ‡ã€‚\n",
    "\n",
    "#### å¼€å§‹è®­ç»ƒ\n",
    "\n",
    "å°†æ•°æ®/æ¨¡å‹/å‚æ•°ä¼ å…¥`Trainer`å³å¯ï¼š"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import Trainer\n",
    "trainer = Trainer(\n",
    "    model=model,\n",
    "    args=training_args,\n",
    "    train_dataset=lm_datasets[\"train\"][:1000],\n",
    "    eval_dataset=lm_datasets[\"validation\"][:1000],\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "è°ƒç”¨`train`æ–¹æ³•å¼€å§‹è®­ç»ƒï¼š"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "trainer.train()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### è¯„ä¼°æ¨¡å‹\n",
    "\n",
    "ä¸€æ—¦è®­ç»ƒå®Œæˆï¼Œæˆ‘ä»¬å°±å¯ä»¥è¯„ä¼°æ¨¡å‹ï¼Œå¾—åˆ°å®ƒåœ¨éªŒè¯é›†ä¸Šçš„perplexityï¼Œå¦‚ä¸‹æ‰€ç¤º:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import math\n",
    "eval_results = trainer.evaluate()\n",
    "print(f\"Perplexity: {math.exp(eval_results['eval_loss']):.2f}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## æ©ç è¯­è¨€æ¨¡å‹ï¼ˆMask Language Modelingï¼ŒMLMï¼‰\n",
    "\n",
    "æ©ç è¯­è¨€æ¨¡å‹ç›¸æ¯”å› æœè¯­è¨€æ¨¡å‹å¥½è®­ç»ƒå¾—å¤šï¼Œå› ä¸ºåªéœ€è¦å¯¹maskçš„token(æ¯”å¦‚åªå æ€»æ•°çš„15%)è¿›è¡Œé¢„æµ‹ï¼ŒåŒæ—¶å¯ä»¥è®¿é—®å…¶ä½™çš„tokenã€‚å¯¹äºæ¨¡å‹æ¥è¯´ï¼Œè¿™æ˜¯ä¸€é¡¹æ›´å®¹æ˜“çš„ä»»åŠ¡ã€‚\n",
    "\n",
    "### æ•°æ®é¢„å¤„ç†\n",
    "\n",
    "å’Œå‰é¢çš„æ­¥éª¤ç›¸åŒï¼š\n",
    "\n",
    "1. åˆ†è¯ï¼›\n",
    "2. è½¬åŒ–æˆå¯¹åº”ä»»åŠ¡è¾“å…¥æ¨¡å‹çš„æ ¼å¼ï¼›\n",
    "\n",
    "#### åˆå§‹åŒ–Tokenizer\n",
    "\n",
    "æˆ‘ä»¬ä½¿ç”¨å·²ç»è®­ç»ƒå¥½çš„[`distilroberta-base`](https://huggingface.co/distilroberta-base)æ¨¡å‹checkpointæ¥åšè¯¥ä»»åŠ¡ã€‚"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import AutoTokenizer\n",
    "model_checkpoint = \"distilroberta-base\"\n",
    "tokenizer = AutoTokenizer.from_pretrained(model_checkpoint, use_fast=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### è½¬åŒ–æˆå¯¹åº”ä»»åŠ¡è¾“å…¥æ¨¡å‹çš„æ ¼å¼\n",
    "\n",
    "å¯¹äº**æ©ç è¯­è¨€æ¨¡å‹(MLM)**ï¼Œæˆ‘ä»¬é¦–å…ˆè·å–åˆ°æ•°æ®é›†ä¸­çš„æ‰€æœ‰æ–‡æœ¬å¹¶åˆ†è¯ï¼Œä¹‹åå°†å®ƒä»¬è¿æ¥èµ·æ¥ï¼Œæ¥ç€åœ¨ç‰¹å®šåºåˆ—é•¿åº¦çš„ä¾‹å­ä¸­æ‹†åˆ†å®ƒä»¬ã€‚ä¸å› æœè¯­è¨€æ¨¡å‹ä¸åŒçš„æ˜¯ï¼Œæˆ‘ä»¬**åœ¨æ‹†åˆ†åè¿˜éœ€è¦éšæœº\"MASK\"ä¸€äº›å­—ç¬¦(ä½¿ç”¨\"[MASK]\"è¿›è¡Œæ›¿æ¢)ä»¥åŠè°ƒæ•´æ ‡ç­¾ä¸ºåªåŒ…å«åœ¨\"[MASK]\"ä½ç½®å¤„çš„æ ‡ç­¾(å› ä¸ºæˆ‘ä»¬ä¸éœ€è¦é¢„æµ‹æ²¡æœ‰è¢«\"MASK\"çš„å­—ç¬¦)**ï¼Œæœ€åå°†å„ä¸ªç»æ©ç çš„æ‹†åˆ†éƒ¨åˆ†ä½œä¸ºæ¨¡å‹è¾“å…¥ã€‚\n",
    "\n",
    "##### è°ƒç”¨åˆ†è¯å™¨å¯¹æ‰€æœ‰çš„æ–‡æœ¬åˆ†è¯\n",
    "\n",
    "åº”ç”¨ä¸€ä¸ªå’Œå‰é¢ç›¸åŒçš„åˆ†è¯å™¨å‡½æ•°ï¼Œåªéœ€è¦æ›´æ–°åˆ†è¯å™¨æ¥ä½¿ç”¨åˆšåˆšé€‰æ‹©çš„checkpointï¼š"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def tokenize_function(examples):\n",
    "    return tokenizer(examples[\"text\"])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "åŒæ ·**ä½¿ç”¨mapå‡½æ•°**å¯¹æ•°æ®é›†**datasetsé‡Œé¢ä¸‰ä¸ªæ ·æœ¬é›†åˆçš„æ‰€æœ‰æ ·æœ¬è¿›è¡Œé¢„å¤„ç†ï¼Œ**å°†å‡½æ•°`tokenize_function`åº”ç”¨åˆ°ï¼ˆmap)æ‰€æœ‰æ ·æœ¬ä¸Šã€‚"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "tokenized_datasets = datasets.map(tokenize_function, batched=True, num_proc=4, remove_columns=[\"text\"])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### æ‹¼æ¥æ–‡æœ¬&æŒ‰å®šé•¿æ‹†åˆ†æ–‡æœ¬\n",
    "\n",
    "æˆ‘ä»¬éœ€è¦å°†æ‰€æœ‰æ–‡æœ¬è¿æ¥åœ¨ä¸€èµ·ï¼Œç„¶åå°†ç»“æœåˆ†å‰²æˆç‰¹å®š`block_size`çš„å°å—ã€‚`block_size`è®¾ç½®ä¸ºé¢„è®­ç»ƒæ¨¡å‹æ—¶æ‰€ä½¿ç”¨çš„æœ€å¤§é•¿åº¦ã€‚\n",
    "\n",
    "ç¼–å†™é¢„å¤„ç†å‡½æ•°æ¥å¯¹æ–‡æœ¬è¿›è¡Œç»„åˆå’Œæ‹†åˆ†ï¼š"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# block_size = tokenizer.model_max_length\n",
    "block_size = 128\n",
    "def group_texts(examples):\n",
    "    # æ‹¼æ¥æ‰€æœ‰æ–‡æœ¬\n",
    "    concatenated_examples = {k: sum(examples[k], []) for k in examples.keys()}\n",
    "    total_length = len(concatenated_examples[list(examples.keys())[0]])\n",
    "    # æˆ‘ä»¬å°†ä½™æ•°å¯¹åº”çš„éƒ¨åˆ†å»æ‰ã€‚ä½†å¦‚æœæ¨¡å‹æ”¯æŒçš„è¯ï¼Œå¯ä»¥æ·»åŠ paddingï¼Œæ‚¨å¯ä»¥æ ¹æ®éœ€è¦å®šåˆ¶æ­¤éƒ¨ä»¶ã€‚\n",
    "    total_length = (total_length // block_size) * block_size\n",
    "    # é€šè¿‡max_lenè¿›è¡Œåˆ†å‰²ã€‚\n",
    "    result = {\n",
    "        k: [t[i : i + block_size] for i in range(0, total_length, block_size)]\n",
    "        for k, t in concatenated_examples.items()\n",
    "    }\n",
    "    result[\"labels\"] = result[\"input_ids\"].copy()\n",
    "    return result"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**æ³¨æ„ï¼šæˆ‘ä»¬è¿™é‡Œä»ç„¶å¤åˆ¶äº†æ ‡ç­¾çš„è¾“å…¥ä½œä¸ºlabelï¼Œå› ä¸ºæ©ç è¯­è¨€æ¨¡å‹çš„æœ¬è´¨è¿˜æ˜¯é¢„æµ‹åŸæ–‡ï¼Œè€Œæ©ç åœ¨data collatorä¸­é€šè¿‡æ·»åŠ ç‰¹åˆ«çš„å‚æ•°è¿›è¡Œå¤„ç†ï¼Œä¸‹æ–‡ä¼šç€é‡è¯´æ˜**ã€‚\n",
    "\n",
    "æˆ‘ä»¬å°†å†æ¬¡ä½¿ç”¨`map`æ–¹æ³•ï¼Œ`batched=True`è¡¨ç¤ºå…è®¸é€šè¿‡è¿”å›ä¸åŒæ•°é‡çš„æ ·æœ¬æ¥æ”¹å˜æ•°æ®é›†ä¸­çš„æ ·æœ¬æ•°é‡ï¼Œè¿™æ ·å¯ä»¥ä»ä¸€æ‰¹ç¤ºä¾‹ä¸­åˆ›å»ºæ–°çš„ç¤ºä¾‹ã€‚\n",
    "\n",
    "æ³¨æ„ï¼Œåœ¨é»˜è®¤æƒ…å†µä¸‹ï¼Œ`map`æ–¹æ³•å°†å‘é€ä¸€æ‰¹1,000ä¸ªç¤ºä¾‹ï¼Œç”±é¢„å¤„ç†å‡½æ•°å¤„ç†ã€‚**å¯ä»¥é€šè¿‡ä¼ é€’ä¸åŒbatch_sizeæ¥è°ƒæ•´ã€‚ä¹Ÿå¯ä»¥ä½¿ç”¨`num_proc `æ¥åŠ é€Ÿé¢„å¤„ç†ã€‚**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "lm_datasets = tokenized_datasets.map(\n",
    "    group_texts,\n",
    "    batched=True,\n",
    "    batch_size=1000,\n",
    "    num_proc=4,\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### å¾®è°ƒé¢„è®­ç»ƒæ¨¡å‹\n",
    "\n",
    "æ•°æ®å·²ç»å‡†å¤‡å¥½äº†ï¼Œæˆ‘ä»¬éœ€è¦ä¸‹è½½å¹¶åŠ è½½é¢„è®­ç»ƒæ¨¡å‹ï¼Œç„¶åå¾®è°ƒé¢„è®­ç»ƒæ¨¡å‹ã€‚\n",
    "\n",
    "#### åŠ è½½é¢„è®­ç»ƒæ¨¡å‹\n",
    "\n",
    "åš**æ©ç è¯­è¨€æ¨¡å‹ä»»åŠ¡ï¼Œé‚£ä¹ˆéœ€è¦ä¸€ä¸ªèƒ½è§£å†³è¿™ä¸ªä»»åŠ¡çš„æ¨¡å‹ç±»ã€‚æˆ‘ä»¬ä½¿ç”¨`AutoModelForMaskedLM` è¿™ä¸ªç±»**ã€‚\n",
    "\n",
    "å’Œä¹‹å‰å‡ ç¯‡åšå®¢æåˆ°çš„åŠ è½½æ–¹å¼ç›¸åŒä¸å†èµ˜è¿°ã€‚"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import AutoModelForMaskedLM\n",
    "model = AutoModelForMaskedLM.from_pretrained(model_checkpoint)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### è®¾å®šè®­ç»ƒå‚æ•°\n",
    "\n",
    "ä¸ºäº†èƒ½å¤Ÿå¾—åˆ°ä¸€ä¸ª**`Trainer`è®­ç»ƒå·¥å…·**ï¼Œæˆ‘ä»¬è¿˜éœ€è¦**è®­ç»ƒçš„è®¾å®š/å‚æ•° [`TrainingArguments`](https://huggingface.co/transformers/main_classes/trainer.html#transformers.TrainingArguments)ã€‚è¿™ä¸ªè®­ç»ƒè®¾å®šåŒ…å«äº†èƒ½å¤Ÿå®šä¹‰è®­ç»ƒè¿‡ç¨‹çš„æ‰€æœ‰å±æ€§**ã€‚"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import TrainingArguments\n",
    "batch_size = 16\n",
    "training_args = TrainingArguments(\n",
    "    \"test-clm\",\n",
    "    evaluation_strategy = \"epoch\",\n",
    "    learning_rate=2e-5,\n",
    "    weight_decay=0.01,\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### æ•°æ®æ”¶é›†å™¨data collator\n",
    "\n",
    "data_collatoræ˜¯ä¸€ä¸ªå‡½æ•°ï¼Œè´Ÿè´£è·å–æ ·æœ¬å¹¶å°†å®ƒä»¬æ‰¹å¤„ç†æˆå¼ é‡ã€‚data_collatorè´Ÿè´£è·å–æ ·æœ¬å¹¶å°†å®ƒä»¬æ‰¹å¤„ç†æˆå¼ é‡ã€‚æ©ç è¯­è¨€æ¨¡å‹ä»»åŠ¡éœ€è¦ä½¿ç”¨ä¸€ä¸ªç‰¹æ®Šçš„data_collatorï¼Œç”¨äºéšæœº\"MASK\"å¥å­ä¸­çš„tokenã€‚\n",
    "\n",
    "æ³¨æ„ï¼šæˆ‘ä»¬å¯ä»¥å°†MASKä½œä¸ºé¢„å¤„ç†æ­¥éª¤(`tokenizer`)è¿›è¡Œå¤„ç†ï¼Œä½†`tokenizer`åœ¨æ¯ä¸ªé˜¶æ®µå­—ç¬¦æ€»æ˜¯ä»¥ç›¸åŒçš„æ–¹å¼è¢«æ©ç›–ã€‚è€Œé€šè¿‡åœ¨`data_collator`ä¸­æ‰§è¡Œè¿™ä¸€æ­¥ï¼Œå¯ä»¥ç¡®ä¿**æ¯æ¬¡ç”Ÿæˆæ•°æ®æ—¶éƒ½ä»¥æ–°çš„æ–¹å¼å®Œæˆæ©ç ï¼ˆéšæœºï¼‰ã€‚**\n",
    "\n",
    "ä¸ºäº†å®ç°éšæœºmaskï¼Œ**`Transformers`ä¸ºæ©ç è¯­è¨€æ¨¡å‹æä¾›äº†ä¸€ä¸ªç‰¹æ®Šçš„`DataCollatorForLanguageModeling`ã€‚**å¯ä»¥é€šè¿‡`mlm_probability`è°ƒæ•´æ©ç çš„æ¦‚ç‡ï¼š"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import DataCollatorForLanguageModeling\n",
    "data_collator = DataCollatorForLanguageModeling(tokenizer=tokenizer, mlm_probability=0.15)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### å®šä¹‰è¯„ä¼°æ–¹æ³•\n",
    "\n",
    "å®Œæˆè¯¥ä»»åŠ¡ä¸éœ€è¦ç‰¹åˆ«å®šä¹‰è¯„ä¼°æŒ‡æ ‡å¤„ç†å‡½æ•°ï¼Œæ¨¡å‹å°†ç›´æ¥è®¡ç®—å›°æƒ‘åº¦perplexityä½œä¸ºè¯„ä¼°æŒ‡æ ‡ã€‚\n",
    "\n",
    "#### å¼€å§‹è®­ç»ƒ\n",
    "\n",
    "å°†æ•°æ®/æ¨¡å‹/å‚æ•°ä¼ å…¥`Trainer`å³å¯ï¼š"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import Trainer\n",
    "trainer = Trainer(\n",
    "    model=model,\n",
    "    args=training_args,\n",
    "    train_dataset=lm_datasets[\"train\"][:1000],\n",
    "    eval_dataset=lm_datasets[\"validation\"][:100],\n",
    "    data_collator=data_collator,\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "è°ƒç”¨`train`æ–¹æ³•å¼€å§‹è®­ç»ƒï¼š"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "trainer.train()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### è¯„ä¼°æ¨¡å‹\n",
    "\n",
    "è®­ç»ƒå®Œæˆåå°±å¯ä»¥è¯„ä¼°æ¨¡å‹ï¼Œå¾—åˆ°å®ƒåœ¨éªŒè¯é›†ä¸Šçš„perplexityï¼Œå¦‚ä¸‹æ‰€ç¤ºï¼š"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import math\n",
    "eval_results = trainer.evaluate()\n",
    "print(f\"Perplexity: {math.exp(eval_results['eval_loss']):.2f}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## å‚è€ƒæ–‡çŒ®\n",
    "\n",
    "[4.5-ç”Ÿæˆä»»åŠ¡-è¯­è¨€æ¨¡å‹.md](https://github.com/datawhalechina/learn-nlp-with-transformers/blob/main/docs/ç¯‡ç« 4-ä½¿ç”¨Transformersè§£å†³NLPä»»åŠ¡/4.5-ç”Ÿæˆä»»åŠ¡-è¯­è¨€æ¨¡å‹.md)\n",
    "\n",
    "[transformerså®˜æ–¹æ–‡æ¡£](https://huggingface.co/transformers)\n",
    "\n",
    "[BERTå®æˆ˜â€”â€”ï¼ˆ1ï¼‰æ–‡æœ¬åˆ†ç±» | å†¬äºçš„åšå®¢ (ifwind.github.io)](https://ifwind.github.io/2021/08/26/BERTå®æˆ˜â€”â€”ï¼ˆ1ï¼‰æ–‡æœ¬åˆ†ç±»/#åŠ è½½è‡ªå·±çš„æ•°æ®æˆ–æ¥è‡ªç½‘ç»œçš„æ•°æ®)\n",
    "\n",
    "[BERTç›¸å…³â€”â€”ï¼ˆ1ï¼‰è¯­è¨€æ¨¡å‹](https://ifwind.github.io/2021/08/20/BERTç›¸å…³â€”â€”ï¼ˆ1ï¼‰è¯­è¨€æ¨¡å‹/)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
