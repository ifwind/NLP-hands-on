{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# BERTå®æˆ˜â€”â€”ï¼ˆ1ï¼‰æ–‡æœ¬åˆ†ç±»\n",
    "\n",
    "## å¼•è¨€\n",
    "\n",
    "æˆ‘ä»¬å°†å±•ç¤ºå¦‚ä½•ä½¿ç”¨ [ğŸ¤— Transformers](https://github.com/huggingface/transformers)ä»£ç åº“ä¸­çš„æ¨¡å‹æ¥è§£å†³æ–‡æœ¬åˆ†ç±»ä»»åŠ¡ï¼Œä»»åŠ¡æ¥æºäº[GLUE Benchmark](https://gluebenchmark.com/).\n",
    "\n",
    "### ä»»åŠ¡ä»‹ç»\n",
    "\n",
    "æœ¬è´¨å°±æ˜¯åˆ†ç±»é—®é¢˜ï¼Œæ¯”å¦‚å¯¹ä¸€å¥è¯çš„æƒ…æ„Ÿææ€§åˆ†ç±»ï¼ˆæ­£å‘1æˆ–è´Ÿå‘-1æˆ–ä¸­æ€§0ï¼‰ï¼š"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "è¾“å…¥ï¼šè¿™éƒ¨ç”µå½±çœŸä¸é”™ï¼\n",
    "è¾“å‡ºï¼š1"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "ä¸»è¦åˆ†ä¸ºä»¥ä¸‹å‡ ä¸ªéƒ¨åˆ†ï¼š\n",
    "\n",
    "1. æ•°æ®åŠ è½½\n",
    "2. æ•°æ®é¢„å¤„ç†\n",
    "3. å¾®è°ƒé¢„è®­ç»ƒæ¨¡å‹ï¼šä½¿ç”¨transformerä¸­çš„`Trainer`æ¥å£å¯¹é¢„è®­ç»ƒæ¨¡å‹è¿›è¡Œå¾®è°ƒï¼›\n",
    "4. è¶…å‚æ•°æœç´¢\n",
    "\n",
    "### å‰æœŸå‡†å¤‡\n",
    "\n",
    "å®‰è£…ä»¥ä¸‹ä¸¤ä¸ªåº“ï¼š"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pip install datasets transformers\n",
    "#transformers==4.9.2\n",
    "#datasets==1.11.0"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## æ•°æ®åŠ è½½\n",
    "\n",
    "### æ•°æ®é›†ä»‹ç»\n",
    "\n",
    "æˆ‘ä»¬ä½¿ç”¨çš„æ˜¯GLUEæ¦œå•çš„æ•°æ®é›†ï¼š"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "GLUE_TASKS = [\"cola\", \"mnli\", \"mnli-mm\", \"mrpc\", \"qnli\", \"qqp\", \"rte\", \"sst2\", \"stsb\", \"wnli\"]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "GLUEæ¦œå•åŒ…å«äº†9ä¸ªå¥å­çº§åˆ«çš„åˆ†ç±»ä»»åŠ¡ï¼Œåˆ†åˆ«æ˜¯ï¼š\n",
    "\n",
    "| åˆ†ç±»ä»»åŠ¡                                                     | ä»»åŠ¡ç›®æ ‡                                                     |\n",
    "| ------------------------------------------------------------ | :----------------------------------------------------------- |\n",
    "| [CoLA](https://nyu-mll.github.io/CoLA/) (Corpus of Linguistic Acceptability) | é‰´åˆ«ä¸€ä¸ªå¥å­æ˜¯å¦è¯­æ³•æ­£ç¡®.                                    |\n",
    "| [MNLI](https://arxiv.org/abs/1704.05426) (Multi-Genre Natural Language Inference) | ç»™å®šä¸€ä¸ªå‡è®¾ï¼Œåˆ¤æ–­å¦ä¸€ä¸ªå¥å­ä¸è¯¥å‡è®¾çš„å…³ç³»ï¼šentails, contradicts æˆ–è€… unrelatedã€‚ |\n",
    "| [MRPC](https://www.microsoft.com/en-us/download/details.aspx?id=52398) (Microsoft Research Paraphrase Corpus) | åˆ¤æ–­ä¸¤ä¸ªå¥å­æ˜¯å¦äº’ä¸ºparaphrasesæ”¹å†™.                         |\n",
    "| [QNLI](https://rajpurkar.github.io/SQuAD-explorer/) (Question-answering Natural Language Inference) | åˆ¤æ–­ç¬¬2å¥æ˜¯å¦åŒ…å«ç¬¬1å¥é—®é¢˜çš„ç­”æ¡ˆã€‚                           |\n",
    "| [QQP](https://data.quora.com/First-Quora-Dataset-Release-Question-Pairs) (Quora Question Pairs2) | åˆ¤æ–­ä¸¤ä¸ªé—®å¥æ˜¯å¦è¯­ä¹‰ç›¸åŒã€‚                                   |\n",
    "| [RTE](https://aclweb.org/aclwiki/Recognizing_Textual_Entailment) (Recognizing Textual Entailment) | åˆ¤æ–­ä¸€ä¸ªå¥å­æ˜¯å¦ä¸å‡è®¾æˆentailå…³ç³»ã€‚                         |\n",
    "| [SST-2](https://nlp.stanford.edu/sentiment/index.html) (Stanford Sentiment Treebank) | åˆ¤æ–­ä¸€ä¸ªå¥å­çš„æƒ…æ„Ÿæ­£è´Ÿå‘.                                    |\n",
    "| [STS-B](http://ixa2.si.ehu.es/stswiki/index.php/STSbenchmark) (Semantic Textual Similarity Benchmark) | åˆ¤æ–­ä¸¤ä¸ªå¥å­çš„ç›¸ä¼¼æ€§ï¼ˆåˆ†æ•°ä¸º1-5åˆ†ï¼‰ã€‚                        |\n",
    "| [WNLI](https://cs.nyu.edu/faculty/davise/papers/WinogradSchemas/WS.html) (Winograd Natural Language Inference) | åˆ¤æ–­ä¸€ä¸ªæœ‰åŒ¿åä»£è¯çš„å¥å­å’Œä¸€ä¸ªæœ‰è¯¥ä»£è¯è¢«æ›¿æ¢çš„å¥å­æ˜¯å¦åŒ…å«ã€‚Determine if a sentence with an anonymous pronoun and a sentence with this pronoun replaced are entailed or not. |\n",
    "\n",
    "### åŠ è½½æ•°æ®\n",
    "\n",
    "ä¸‹é¢ä»‹ç»ä¸¤ç§ä½¿ç”¨[ğŸ¤— Datasets](https://github.com/huggingface/datasets)åº“æ¥**åŠ è½½æ•°æ®`load_dataset`**çš„æ–¹æ³•ï¼Œä¸»è¦å‚è€ƒ[å®˜æ–¹æ–‡æ¡£](https://huggingface.co/docs/datasets/loading_datasets.html#from-local-files)ï¼š\n",
    "\n",
    "1. åŠ è½½å®˜æ–¹åº“çš„æ•°æ®ï¼›\n",
    "2. åŠ è½½è‡ªå·±çš„æ•°æ®æˆ–æ¥è‡ªç½‘ç»œçš„æ•°æ®ï¼š\n",
    "   1. csvæ ¼å¼ï¼›\n",
    "   2. jsonæ ¼å¼ï¼›\n",
    "   3. txtæ ¼å¼\n",
    "   4. pandas.DataFrameæ ¼å¼ã€‚\n",
    "\n",
    "#### åŠ è½½å®˜æ–¹åº“çš„æ•°æ®\n",
    "\n",
    "é™¤äº†`mnli-mm`ä»¥å¤–ï¼Œå…¶ä»–ä»»åŠ¡éƒ½å¯ä»¥ç›´æ¥é€šè¿‡ä»»åŠ¡åå­—è¿›è¡ŒåŠ è½½ã€‚æ•°æ®åŠ è½½ä¹‹åä¼šè‡ªåŠ¨ç¼“å­˜ã€‚"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from datasets import load_dataset\n",
    "actual_task = \"mnli\" if task == \"mnli-mm\" else task\n",
    "dataset = load_dataset(\"glue\", actual_task)\n",
    "metric = load_metric('glue', actual_task)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "è¿™ä¸ª`datasets`å¯¹è±¡æœ¬èº«æ˜¯ä¸€ç§[`DatasetDict`](https://huggingface.co/docs/datasets/package_reference/main_classes.html#datasetdict)æ•°æ®ç»“æ„. å¯¹äº**è®­ç»ƒé›†ã€éªŒè¯é›†å’Œæµ‹è¯•é›†ï¼Œåªéœ€è¦ä½¿ç”¨å¯¹åº”çš„`keyï¼ˆtrainï¼Œvalidationï¼Œtestï¼‰`å³å¯å¾—åˆ°ç›¸åº”çš„æ•°æ®**ã€‚\n",
    "\n",
    "ç»™å®šä¸€ä¸ªæ•°æ®åˆ‡åˆ†çš„keyï¼ˆtrainã€validationæˆ–è€…testï¼‰å’Œä¸‹æ ‡å³å¯æŸ¥çœ‹æ•°æ®ï¼š`dataset[\"train\"][0]`\n",
    "\n",
    "ä¸‹é¢çš„å‡½æ•°å°†ä»æ•°æ®é›†é‡Œéšæœºé€‰æ‹©å‡ ä¸ªä¾‹å­è¿›è¡Œå±•ç¤ºï¼š"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import datasets\n",
    "import random\n",
    "import pandas as pd\n",
    "from IPython.display import display, HTML\n",
    "\n",
    "def show_random_elements(dataset, num_examples=10):\n",
    "    assert num_examples <= len(dataset), \"Can't pick more elements than there are in the dataset.\"\n",
    "    picks = []\n",
    "    for _ in range(num_examples):\n",
    "        pick = random.randint(0, len(dataset)-1)\n",
    "        while pick in picks:\n",
    "            pick = random.randint(0, len(dataset)-1)\n",
    "        picks.append(pick)\n",
    "    \n",
    "    df = pd.DataFrame(dataset[picks])\n",
    "    for column, typ in dataset.features.items():\n",
    "        if isinstance(typ, datasets.ClassLabel):\n",
    "            df[column] = df[column].transform(lambda i: typ.names[i])\n",
    "    display(HTML(df.to_html()))\n",
    "    \n",
    "show_random_elements(dataset[\"train\"])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### åŠ è½½è‡ªå·±çš„æ•°æ®æˆ–æ¥è‡ªç½‘ç»œçš„æ•°æ®\n",
    "\n",
    "##### csvæ ¼å¼\n",
    "\n",
    "data_filesä¸ºæœ¬åœ°æ–‡ä»¶åæˆ–ç½‘ç»œæ•°æ®é“¾æ¥ï¼Œå¦‚æœæ²¡æœ‰ç”¨å­—å…¸æŒ‡å®šè®­ç»ƒé›†ã€éªŒè¯é›†ã€æµ‹è¯•é›†ï¼Œé»˜è®¤éƒ½ä¸ºè®­ç»ƒé›†ã€‚"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from datasets import load_dataset\n",
    "dataset = load_dataset('csv', data_files='my_file.csv')\n",
    "dataset = load_dataset('csv', data_files=['my_file_1.csv', 'my_file_2.csv', 'my_file_3.csv'])\n",
    "dataset = load_dataset('csv', data_files={'train': ['my_train_file_1.csv', 'my_train_file_2.csv'],\n",
    "base_url = 'https://huggingface.co/datasets/lhoestq/demo1/resolve/main/data/'\n",
    "dataset = load_dataset('csv', data_files={'train': base_url + 'train.csv', 'test': base_url + 'test.csv'})"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### jsonæ ¼å¼\n",
    "\n",
    "**æƒ…å†µ1**ï¼šjsonæ•°æ®ä¸åŒ…æ‹¬åµŒå¥—çš„jsonï¼Œæ¯”å¦‚ï¼š"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "{\"a\": 1, \"b\": 2.0, \"c\": \"foo\", \"d\": false}\n",
    "{\"a\": 4, \"b\": -5.5, \"c\": null, \"d\": true}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "æ­¤æ—¶å¯ä»¥ç›´æ¥åŠ è½½æ•°æ®ï¼š"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from datasets import load_dataset\n",
    "dataset = load_dataset('json', data_files={'train': ['my_text_1.json', 'my_text_2.json'], 'test': 'my_test_file.json'})\n",
    "\n",
    "dataset = load_dataset('text', data_files={'train': 'https://huggingface.co/datasets/lhoestq/test/resolve/main/some_text.json'})"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**æƒ…å†µ2**ï¼šjsonæ•°æ®åŒ…æ‹¬åµŒå¥—çš„jsonï¼Œæ¯”å¦‚ï¼š"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "{\"version\": \"0.1.0\",\n",
    " \"data\": [{\"a\": 1, \"b\": 2.0, \"c\": \"foo\", \"d\": false},\n",
    "          {\"a\": 4, \"b\": -5.5, \"c\": null, \"d\": true}]\n",
    "}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "æ­¤æ—¶éœ€è¦ä½¿ç”¨ `field` å‚æ•°æŒ‡å®šå“ªä¸ªå­—æ®µåŒ…å«æ•°æ®é›†ï¼š"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from datasets import load_dataset\n",
    "dataset = load_dataset('json', data_files='my_file.json', field='data')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### txtæ ¼å¼"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from datasets import load_dataset\n",
    "dataset = load_dataset('text', data_files={'train': ['my_text_1.txt', 'my_text_2.txt'], 'test': 'my_test_file.txt'})\n",
    "\n",
    "dataset = load_dataset('text', data_files={'train': 'https://huggingface.co/datasets/lhoestq/test/resolve/main/some_text.txt'})"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### dictæ ¼å¼"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "my_dict = {'id': [0, 1, 2],\n",
    "           'name': ['mary', 'bob', 'eve'],\n",
    "           'age': [24, 53, 19]}\n",
    "from datasets import Dataset\n",
    "dataset = Dataset.from_dict(my_dict)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### pandas.DataFrameæ ¼å¼"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from datasets import Dataset\n",
    "import pandas as pd\n",
    "df = pd.DataFrame({\"a\": [1, 2, 3]})\n",
    "dataset = Dataset.from_pandas(df)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## æ•°æ®é¢„å¤„ç†\n",
    "\n",
    "åœ¨å°†æ•°æ®å–‚å…¥æ¨¡å‹ä¹‹å‰ï¼Œæˆ‘ä»¬éœ€è¦å¯¹æ•°æ®è¿›è¡Œé¢„å¤„ç†ã€‚ä¹‹å‰æˆ‘ä»¬å·²ç»çŸ¥é“äº†æ•°æ®é¢„å¤„ç†çš„åŸºæœ¬æµç¨‹ï¼š\n",
    "\n",
    "1. åˆ†è¯ï¼›\n",
    "2. è½¬åŒ–æˆå¯¹åº”ä»»åŠ¡è¾“å…¥æ¨¡å‹çš„æ ¼å¼ï¼›\n",
    "\n",
    "`Tokenizer`ç”¨äºä¸Šé¢ä¸¤æ­¥æ•°æ®é¢„å¤„ç†å·¥ä½œï¼š`Tokenizer`é¦–å…ˆå¯¹è¾“å…¥è¿›è¡Œtokenizeï¼Œç„¶åå°†tokensè½¬åŒ–ä¸ºé¢„æ¨¡å‹ä¸­éœ€è¦å¯¹åº”çš„token IDï¼Œå†è½¬åŒ–ä¸ºæ¨¡å‹éœ€è¦çš„è¾“å…¥æ ¼å¼ã€‚\n",
    "\n",
    "### åˆå§‹åŒ–Tokenizer\n",
    "\n",
    "**ä½¿ç”¨`AutoTokenizer.from_pretrained`æ–¹æ³•æ ¹æ®æ¨¡å‹æ–‡ä»¶å®ä¾‹åŒ–tokenizer**ï¼Œè¿™æ ·å¯ä»¥ç¡®ä¿ï¼š\n",
    "\n",
    "- å¾—åˆ°ä¸€ä¸ª**ä¸é¢„è®­ç»ƒæ¨¡å‹ä¸€ä¸€å¯¹åº”çš„tokenizer**ã€‚\n",
    "- ä½¿ç”¨æŒ‡å®šçš„æ¨¡å‹checkpointå¯¹åº”çš„tokenizeræ—¶ï¼ŒåŒæ—¶ä¸‹è½½äº†æ¨¡å‹éœ€è¦çš„è¯è¡¨åº“vocabularyï¼Œå‡†ç¡®æ¥è¯´æ˜¯tokens vocabularyã€‚"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import AutoTokenizer\n",
    "tokenizer = AutoTokenizer.from_pretrained(model_checkpoint, use_fast=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "æ³¨æ„ï¼š**`use_fast=True`è¦æ±‚tokenizerå¿…é¡»æ˜¯transformers.PreTrainedTokenizerFastç±»å‹**ï¼Œä»¥ä¾¿åœ¨é¢„å¤„ç†çš„æ—¶å€™éœ€è¦ç”¨åˆ°fast tokenizerçš„ä¸€äº›ç‰¹æ®Šç‰¹æ€§ï¼ˆæ¯”å¦‚å¤šçº¿ç¨‹å¿«é€Ÿtokenizerï¼‰ã€‚**å¦‚æœå¯¹åº”çš„æ¨¡å‹æ²¡æœ‰fast tokenizerï¼Œå»æ‰è¿™ä¸ªé€‰é¡¹å³å¯ã€‚**\n",
    "\n",
    "å‡ ä¹æ‰€æœ‰æ¨¡å‹å¯¹åº”çš„tokenizeréƒ½æœ‰å¯¹åº”çš„fast tokenizerï¼Œå¯ä»¥åœ¨[æ¨¡å‹tokenizerå¯¹åº”è¡¨](https://huggingface.co/transformers/index.html#bigtable)é‡ŒæŸ¥çœ‹æ‰€æœ‰é¢„è®­ç»ƒæ¨¡å‹å¯¹åº”çš„tokenizeræ‰€æ‹¥æœ‰çš„ç‰¹ç‚¹ã€‚\n",
    "\n",
    "### Tokenizeråˆ†è¯ç¤ºä¾‹\n",
    "\n",
    "é¢„è®­ç»ƒçš„Tokenizeré€šå¸¸åŒ…å«äº†**åˆ†å•å¥**å’Œ**åˆ†ä¸€å¯¹å¥å­**çš„å‡½æ•°ã€‚å¦‚ï¼š"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#åˆ†å•å¥ï¼ˆä¸€ä¸ªbatchï¼‰\n",
    "batch_sentences = [\"Hello I'm a single sentence\",\n",
    "                   \"And another sentence\",\n",
    "                   \"And the very very last one\"]\n",
    "encoded_inputs = tokenizer(batch_sentences)\n",
    "print(encoded_inputs)\n",
    "#{'input_ids': [[101, 8667, 146, 112, 182, 170, 1423, 5650, 102],\n",
    "#               [101, 1262, 1330, 5650, 102],\n",
    "#               [101, 1262, 1103, 1304, 1304, 1314, 1141, 102]],\n",
    "# 'token_type_ids': [[0, 0, 0, 0, 0, 0, 0, 0, 0],\n",
    "#                    [0, 0, 0, 0, 0],\n",
    "#                    [0, 0, 0, 0, 0, 0, 0, 0]],\n",
    "# 'attention_mask': [[1, 1, 1, 1, 1, 1, 1, 1, 1],\n",
    "#                    [1, 1, 1, 1, 1],\n",
    "#                    [1, 1, 1, 1, 1, 1, 1, 1]]}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#åˆ†ä¸€å¯¹å¥å­\n",
    "encoded_input = tokenizer(\"How old are you?\", \"I'm 6 years old\")\n",
    "print(encoded_input)\n",
    "#{'input_ids': [101, 1731, 1385, 1132, 1128, 136, 102, 146, 112, 182, 127, #1201, 1385, 102],\n",
    "# 'token_type_ids': [0, 0, 0, 0, 0, 0, 0, 1, 1, 1, 1, 1, 1, 1],\n",
    "# 'attention_mask': [1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1]}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "æˆ‘ä»¬ä¹‹å‰ä¹Ÿæåˆ°å¦‚æœæ˜¯**è‡ªå·±é¢„è®­ç»ƒçš„tokenizers**å¯ä»¥é€šè¿‡ä»¥ä¸‹æ–¹å¼ä¸ºtokenizerså¢åŠ å¤„ç†ä¸€å¯¹å¥å­çš„æ–¹æ³•ï¼š"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from tokenizers.processors import TemplateProcessing\n",
    "\n",
    "tokenizer.post_processor = TemplateProcessing(\n",
    "    single=\"[CLS] $A [SEP]\",\n",
    "    pair=\"[CLS] $A [SEP] $B:1 [SEP]:1\",\n",
    "    special_tokens=[\n",
    "        (\"[CLS]\", tokenizer.token_to_id(\"[CLS]\")),\n",
    "        (\"[SEP]\", tokenizer.token_to_id(\"[SEP]\")),\n",
    "    ],\n",
    ")\n",
    "#è®¾ç½®å¥å­æœ€å¤§é•¿åº¦\n",
    "tokenizer.enable_truncation(max_length=512)\n",
    "#ä½¿ç”¨tokenizer.save()ä¿å­˜æ¨¡å‹\n",
    "tokenizer.save(\"data/tokenizer-wiki.json\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### è½¬åŒ–æˆå¯¹åº”ä»»åŠ¡è¾“å…¥æ¨¡å‹çš„æ ¼å¼\n",
    "\n",
    "tokenizeræœ‰ä¸åŒçš„è¿”å›å–å†³äºé€‰æ‹©çš„é¢„è®­ç»ƒæ¨¡å‹ï¼Œ**tokenizerå’Œé¢„è®­ç»ƒæ¨¡å‹æ˜¯ä¸€ä¸€å¯¹åº”çš„**ï¼Œæ›´å¤šä¿¡æ¯å¯ä»¥åœ¨[è¿™é‡Œ](https://huggingface.co/transformers/preprocessing.html)è¿›è¡Œå­¦ä¹ ã€‚\n",
    "\n",
    "**ä¸åŒæ•°æ®å’Œå¯¹åº”çš„æ•°æ®æ ¼å¼**ï¼Œä¸ºäº†é¢„å¤„ç†æˆ‘ä»¬çš„æ•°æ®ï¼Œå®šä¹‰ä¸‹é¢è¿™ä¸ªdictï¼Œä»¥ä¾¿åˆ†åˆ«ç”¨tokenizerå¤„ç†è¾“å…¥æ˜¯å•å¥æˆ–å¥å­å¯¹çš„æƒ…å†µã€‚"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "task_to_keys = {\n",
    "    \"cola\": (\"sentence\", None),\n",
    "    \"mnli\": (\"premise\", \"hypothesis\"),\n",
    "    \"mnli-mm\": (\"premise\", \"hypothesis\"),\n",
    "    \"mrpc\": (\"sentence1\", \"sentence2\"),\n",
    "    \"qnli\": (\"question\", \"sentence\"),\n",
    "    \"qqp\": (\"question1\", \"question2\"),\n",
    "    \"rte\": (\"sentence1\", \"sentence2\"),\n",
    "    \"sst2\": (\"sentence\", None),\n",
    "    \"stsb\": (\"sentence1\", \"sentence2\"),\n",
    "    \"wnli\": (\"sentence1\", \"sentence2\"),\n",
    "}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "å°†é¢„å¤„ç†çš„ä»£ç æ”¾åˆ°ä¸€ä¸ªå‡½æ•°ä¸­ï¼š"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def preprocess_function(examples):\n",
    "    if sentence2_key is None:\n",
    "        return tokenizer(examples[sentence1_key], truncation=True)\n",
    "    return tokenizer(examples[sentence1_key], examples[sentence2_key], truncation=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "å‰é¢æˆ‘ä»¬å·²ç»å±•ç¤ºäº†tokenizerå¤„ç†ä¸€ä¸ªå°batchçš„æ¡ˆä¾‹ã€‚datasetç±»ç›´æ¥ç”¨ç´¢å¼•å°±å¯ä»¥å–å¯¹åº”ä¸‹æ ‡çš„å¥å­1å’Œå¥å­2ï¼Œå› æ­¤ä¸Šé¢çš„**é¢„å¤„ç†å‡½æ•°æ—¢å¯ä»¥å¤„ç†å•ä¸ªæ ·æœ¬ï¼Œä¹Ÿå¯ä»¥å¯¹å¤šä¸ªæ ·æœ¬è¿›è¡Œå¤„ç†ã€‚**å¦‚æœè¾“å…¥æ˜¯å¤šä¸ªæ ·æœ¬ï¼Œé‚£ä¹ˆè¿”å›çš„æ˜¯ä¸€ä¸ªlistï¼š"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "preprocess_function(dataset['train'][:5])\n",
    "#{'input_ids': [[101, 2256, 2814, 2180, 1005, 1056, 4965, 2023, 4106, 1010, 2292, 2894, 1996, 2279, 2028, 2057, 16599, 1012, 102], [101, 2028, 2062, 18404, 2236, 3989, 1998, 1045, 1005, 1049, 3228, 2039, 1012, 102], [101, 2028, 2062, 18404, 2236, 3989, 2030, 1045, 1005, 1049, 3228, 2039, 1012, 102], [101, 1996, 2062, 2057, 2817, 16025, 1010, 1996, 13675, 16103, 2121, 2027, 2131, 1012, 102], [101, 2154, 2011, 2154, 1996, 8866, 2024, 2893, 14163, 8024, 3771, 1012, 102]], 'attention_mask': [[1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1], [1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1], [1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1], [1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1], [1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1]]}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "æ¥ä¸‹æ¥**ä½¿ç”¨mapå‡½æ•°**å¯¹æ•°æ®é›†**datasetsé‡Œé¢ä¸‰ä¸ªæ ·æœ¬é›†åˆçš„æ‰€æœ‰æ ·æœ¬è¿›è¡Œé¢„å¤„ç†ï¼Œ**å°†é¢„å¤„ç†å‡½æ•°prepare_train_featuresåº”ç”¨åˆ°ï¼ˆmap)æ‰€æœ‰æ ·æœ¬ä¸Šã€‚"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "encoded_dataset = dataset.map(preprocess_function, batched=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "> **è¿”å›çš„ç»“æœä¼šè‡ªåŠ¨è¢«ç¼“å­˜ï¼Œé¿å…ä¸‹æ¬¡å¤„ç†çš„æ—¶å€™é‡æ–°è®¡ç®—ï¼ˆä½†æ˜¯ä¹Ÿè¦æ³¨æ„ï¼Œå¦‚æœè¾“å…¥æœ‰æ”¹åŠ¨ï¼Œå¯èƒ½ä¼šè¢«ç¼“å­˜å½±å“ï¼ï¼‰**ã€‚\n",
    ">\n",
    "> datasetsåº“å‡½æ•°ä¼šå¯¹è¾“å…¥çš„å‚æ•°è¿›è¡Œæ£€æµ‹ï¼Œåˆ¤æ–­æ˜¯å¦æœ‰å˜åŒ–ï¼Œå¦‚æœæ²¡æœ‰å˜åŒ–å°±ä½¿ç”¨ç¼“å­˜æ•°æ®ï¼Œå¦‚æœæœ‰å˜åŒ–å°±é‡æ–°å¤„ç†ã€‚ä½†å¦‚æœè¾“å…¥å‚æ•°ä¸å˜ï¼Œæƒ³æ”¹å˜è¾“å…¥çš„æ—¶å€™ï¼Œæœ€å¥½æ¸…ç†è°ƒè¿™ä¸ªç¼“å­˜**ï¼ˆä½¿ç”¨`load_from_cache_file=False`å‚æ•°ï¼‰**ã€‚å¦å¤–ï¼Œä¸Šé¢ä½¿ç”¨åˆ°çš„**`batched=True`è¿™ä¸ªå‚æ•°æ˜¯tokenizerçš„ç‰¹ç‚¹ï¼Œè¿™ä¼šä½¿ç”¨å¤šçº¿ç¨‹åŒæ—¶å¹¶è¡Œå¯¹è¾“å…¥è¿›è¡Œå¤„ç†ã€‚**\n",
    "\n",
    "## å¾®è°ƒé¢„è®­ç»ƒæ¨¡å‹\n",
    "\n",
    "æ•°æ®å·²ç»å‡†å¤‡å¥½äº†ï¼Œæˆ‘ä»¬éœ€è¦ä¸‹è½½å¹¶åŠ è½½é¢„è®­ç»ƒæ¨¡å‹ï¼Œç„¶åå¾®è°ƒé¢„è®­ç»ƒæ¨¡å‹ã€‚\n",
    "\n",
    "### åŠ è½½é¢„è®­ç»ƒæ¨¡å‹\n",
    "\n",
    "æ—¢ç„¶æ˜¯åš**seq2seqä»»åŠ¡ï¼Œé‚£ä¹ˆéœ€è¦ä¸€ä¸ªèƒ½è§£å†³è¿™ä¸ªä»»åŠ¡çš„æ¨¡å‹ç±»ã€‚æˆ‘ä»¬ä½¿ç”¨`AutoModelForSequenceClassification` è¿™ä¸ªç±»**ã€‚\n",
    "\n",
    "å’Œtokenizerç›¸ä¼¼ï¼Œ`from_pretrained`æ–¹æ³•åŒæ ·å¯ä»¥å¸®åŠ©ä¸‹è½½å¹¶åŠ è½½æ¨¡å‹ï¼ŒåŒæ—¶ä¹Ÿä¼šå¯¹æ¨¡å‹è¿›è¡Œç¼“å­˜ï¼Œä¹Ÿå¯ä»¥å¡«å…¥ä¸€ä¸ªåŒ…æ‹¬æ¨¡å‹ç›¸å…³æ–‡ä»¶çš„æ–‡ä»¶å¤¹ï¼ˆæ¯”å¦‚è‡ªå·±é¢„è®­ç»ƒçš„æ¨¡å‹ï¼‰ï¼Œè¿™æ ·ä¼šä»æœ¬åœ°ç›´æ¥åŠ è½½ã€‚ç†è®ºä¸Šå¯ä»¥ä½¿ç”¨å„ç§å„æ ·çš„transformeræ¨¡å‹ï¼ˆ[æ¨¡å‹é¢æ¿](https://huggingface.co/models)ï¼‰ï¼Œè§£å†³ä»»ä½•æ–‡æœ¬åˆ†ç±»åˆ†ç±»ä»»åŠ¡ã€‚\n",
    "\n",
    "éœ€è¦æ³¨æ„çš„æ˜¯ï¼š**STS-Bæ˜¯ä¸€ä¸ªå›å½’é—®é¢˜ï¼ŒMNLIæ˜¯ä¸€ä¸ª3åˆ†ç±»é—®é¢˜**ï¼š"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import AutoModelForSequenceClassification, TrainingArguments, Trainer\n",
    "\n",
    "task = \"cola\"\n",
    "model_checkpoint = \"distilbert-base-uncased\" #æ‰€é€‰æ‹©çš„é¢„è®­ç»ƒæ¨¡å‹\n",
    "\n",
    "num_labels = 3 if task.startswith(\"mnli\") else 1 if task==\"stsb\" else 2\n",
    "model = AutoModelForSequenceClassification.from_pretrained(model_checkpoint, num_labels=num_labels)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**ç”±äºæˆ‘ä»¬çš„ä»»åŠ¡æ˜¯æ–‡æœ¬åˆ†ç±»ä»»åŠ¡ï¼Œè€Œæˆ‘ä»¬åŠ è½½çš„æ˜¯é¢„è®­ç»ƒè¯­è¨€æ¨¡å‹**ï¼Œæ‰€ä»¥**ä¼šæç¤ºæˆ‘ä»¬åŠ è½½æ¨¡å‹çš„æ—¶å€™æ‰”æ‰äº†ä¸€äº›ä¸åŒ¹é…çš„ç¥ç»ç½‘ç»œå‚æ•°**ï¼ˆæ¯”å¦‚ï¼šé¢„è®­ç»ƒè¯­è¨€æ¨¡å‹çš„ç¥ç»ç½‘ç»œheadè¢«æ‰”æ‰äº†ï¼ŒåŒæ—¶éšæœºåˆå§‹åŒ–äº†æ–‡æœ¬åˆ†ç±»çš„ç¥ç»ç½‘ç»œheadï¼‰ã€‚\n",
    "\n",
    "### è®¾å®šè®­ç»ƒå‚æ•°\n",
    "\n",
    "ä¸ºäº†èƒ½å¤Ÿå¾—åˆ°ä¸€ä¸ª`Trainer`è®­ç»ƒå·¥å…·ï¼Œæˆ‘ä»¬è¿˜éœ€è¦**è®­ç»ƒçš„è®¾å®š/å‚æ•° [`TrainingArguments`](https://huggingface.co/transformers/main_classes/trainer.html#transformers.TrainingArguments)ã€‚è¿™ä¸ªè®­ç»ƒè®¾å®šåŒ…å«äº†èƒ½å¤Ÿå®šä¹‰è®­ç»ƒè¿‡ç¨‹çš„æ‰€æœ‰å±æ€§**ã€‚"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "batch_size = 16\n",
    "metric_name = \"pearson\" if task == \"stsb\" else \"matthews_correlation\" if task == \"cola\" else \"accuracy\"\n",
    "\n",
    "args = TrainingArguments(\n",
    "    \"test-glue\",\n",
    "    evaluation_strategy = \"epoch\", #æ¯ä¸ªepcohä¼šåšä¸€æ¬¡éªŒè¯è¯„ä¼°ï¼›\n",
    "    save_strategy = \"epoch\",\n",
    "    learning_rate=2e-5,\n",
    "    per_device_train_batch_size=batch_size,\n",
    "    per_device_eval_batch_size=batch_size,\n",
    "    num_train_epochs=5,\n",
    "    weight_decay=0.01,\n",
    "    load_best_model_at_end=True,\n",
    "    metric_for_best_model=metric_name, #æ ¹æ®å“ªä¸ªè¯„ä»·æŒ‡æ ‡é€‰æœ€ä¼˜æ¨¡å‹\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### å®šä¹‰è¯„ä¼°æ–¹æ³•\n",
    "\n",
    "è¿˜æœ‰ä¸€ä»¶é‡è¦çš„äº‹ï¼Œæˆ‘ä»¬éœ€è¦**é€‰æ‹©ä¸€ä¸ªåˆé€‚çš„è¯„ä»·æŒ‡æ ‡å¼•å¯¼æ¨¡å‹è¿›è¡Œå¾®è°ƒ**ã€‚\n",
    "\n",
    "æˆ‘ä»¬ä½¿ç”¨[ğŸ¤— Datasets](https://github.com/huggingface/datasets)åº“æ¥**åŠ è½½è¯„ä»·æŒ‡æ ‡è®¡ç®—åº“`load_metric`**ã€‚meticæ˜¯[`datasets.Metric`](https://huggingface.co/docs/datasets/package_reference/main_classes.html#datasets.Metric)çš„ä¸€ä¸ªå®ä¾‹:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from datasets import load_metric"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "ç›´æ¥è°ƒç”¨metricçš„`compute`æ–¹æ³•ï¼Œä¼ å…¥`labels`å’Œ`predictions`å³å¯å¾—åˆ°metricçš„å€¼ï¼š"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "\n",
    "fake_preds = np.random.randint(0, 2, size=(64,))\n",
    "fake_labels = np.random.randint(0, 2, size=(64,))\n",
    "metric.compute(predictions=fake_preds, references=fake_labels)\n",
    "#{'matthews_correlation': 0.1513518081969605}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**æ¯ä¸€ä¸ªæ–‡æœ¬åˆ†ç±»ä»»åŠ¡æ‰€å¯¹åº”çš„meticæœ‰æ‰€ä¸åŒ**ï¼Œä¸€å®š**è¦å°†metricå’Œä»»åŠ¡å¯¹é½**ï¼Œå…·ä½“å¦‚ä¸‹:\n",
    "\n",
    "| GLUE benchmarkåˆ†ç±»ä»»åŠ¡       | è¯„ä»·æŒ‡æ ‡                                                     |\n",
    "| ---------------------------- | ------------------------------------------------------------ |\n",
    "| CoLA                         | [Matthews Correlation Coefficient](https://en.wikipedia.org/wiki/Matthews_correlation_coefficient) |\n",
    "| MNLI (matched or mismatched) | Accuracy                                                     |\n",
    "| MRPC                         | Accuracy and [F1 score](https://en.wikipedia.org/wiki/F1_score) |\n",
    "| QNLI                         | Accuracy                                                     |\n",
    "| QQP                          | Accuracy and [F1 score](https://en.wikipedia.org/wiki/F1_score) |\n",
    "| RTE                          | Accuracy                                                     |\n",
    "| SST-2                        | Accuracy                                                     |\n",
    "| STS-B                        | [Pearson Correlation Coefficient](https://en.wikipedia.org/wiki/Pearson_correlation_coefficient) and [Spearman's_Rank_Correlation_Coefficient](https://en.wikipedia.org/wiki/Spearman%27s_rank_correlation_coefficient) |\n",
    "| WNLI                         | Accuracy                                                     |\n",
    "\n",
    "ä¸º`Trainer`å®šä¹‰å„ä¸ªä»»åŠ¡çš„è¯„ä¼°æ–¹æ³•`compute_metrics`ï¼š"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from datasets import load_metric\n",
    "def compute_metrics(eval_pred):\n",
    "    predictions, labels = eval_pred\n",
    "    if task != \"stsb\":\n",
    "        predictions = np.argmax(predictions, axis=1)\n",
    "    else:\n",
    "        predictions = predictions[:, 0]\n",
    "    return metric.compute(predictions=predictions, references=labels)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### å¼€å§‹è®­ç»ƒ\n",
    "\n",
    "å°†æ•°æ®/æ¨¡å‹/å‚æ•°ä¼ å…¥`Trainer`å³å¯ï¼š"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "validation_key = \"validation_mismatched\" if task == \"mnli-mm\" else \"validation_matched\" if task == \"mnli\" else \"validation\"\n",
    "trainer = Trainer(\n",
    "    model,\n",
    "    args,\n",
    "    train_dataset=encoded_dataset[\"train\"],\n",
    "    eval_dataset=encoded_dataset[validation_key],\n",
    "    tokenizer=tokenizer,\n",
    "    compute_metrics=compute_metrics\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "å¼€å§‹è®­ç»ƒ:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "trainer.train()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### æ¨¡å‹è¯„ä¼°\n",
    "\n",
    "è®­ç»ƒå®Œæˆåå¯¹æ¨¡å‹è¿›è¡Œè¯„ä¼°ï¼š"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "trainer.evaluate()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## è¶…å‚æ•°æœç´¢\n",
    "\n",
    "**`Trainer`è¿˜æ”¯æŒè¶…å‚æœç´¢ï¼Œä½¿ç”¨[optuna](https://optuna.org/) or [Ray Tune](https://docs.ray.io/en/latest/tune/)ä»£ç åº“ã€‚**\n",
    "\n",
    "éœ€è¦å®‰è£…ä»¥ä¸‹ä¸¤ä¸ªä¾èµ–ï¼š"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pip install optuna\n",
    "pip install ray[tune]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "è¶…å‚æœç´¢æ—¶ï¼Œ`Trainer`å°†ä¼šè¿”å›å¤šä¸ªè®­ç»ƒå¥½çš„æ¨¡å‹ï¼Œæ‰€ä»¥**éœ€è¦ä¼ å…¥ä¸€ä¸ªå®šä¹‰å¥½çš„æ¨¡å‹ä»è€Œè®©`Trainer`å¯ä»¥ä¸æ–­é‡æ–°åˆå§‹åŒ–è¯¥ä¼ å…¥çš„æ¨¡å‹ï¼š**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def model_init():\n",
    "    return AutoModelForSequenceClassification.from_pretrained(model_checkpoint, num_labels=num_labels)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "å’Œä¹‹å‰è°ƒç”¨ `Trainer`ç±»ä¼¼:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "trainer = Trainer(\n",
    "    model_init=model_init,\n",
    "    args=args,\n",
    "    train_dataset=encoded_dataset[\"train\"],\n",
    "    eval_dataset=encoded_dataset[validation_key],\n",
    "    tokenizer=tokenizer,\n",
    "    compute_metrics=compute_metrics\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**è°ƒç”¨æ–¹æ³•`hyperparameter_search`è¿›è¡Œè¶…å‚æ•°æœç´¢**ã€‚\n",
    "\n",
    "**æ³¨æ„ï¼Œè¿™ä¸ªè¿‡ç¨‹å¯èƒ½å¾ˆä¹…ï¼Œå¯ä»¥å…ˆç”¨éƒ¨åˆ†æ•°æ®é›†è¿›è¡Œè¶…å‚æœç´¢ï¼Œå†è¿›è¡Œå…¨é‡è®­ç»ƒã€‚**\n",
    "æ¯”å¦‚**ä½¿ç”¨1/10çš„æ•°æ®è¿›è¡Œæœç´¢**ï¼ˆåˆ©ç”¨`n_trials`è®¾ç½®ï¼‰ï¼š"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "best_run = trainer.hyperparameter_search(n_trials=10, direction=\"maximize\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**`hyperparameter_search`ä¼šè¿”å›æ•ˆæœæœ€å¥½çš„æ¨¡å‹ç›¸å…³çš„å‚æ•°best_run**ï¼š\n",
    "\n",
    "**å°†`Trainner`è®¾ç½®ä¸ºæœç´¢åˆ°çš„æœ€å¥½å‚æ•°best_run**ï¼Œå†å¯¹å…¨éƒ¨æ•°æ®è¿›è¡Œè®­ç»ƒï¼š"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for n, v in best_run.hyperparameters.items():\n",
    "    setattr(trainer.args, n, v)\n",
    "\n",
    "trainer.train()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## ä¸Šä¼ æ¨¡å‹åˆ°huggingface\n",
    "\n",
    "[å­¦ä¹ å¦‚ä½•ä¸Šä¼ æ¨¡å‹](https://huggingface.co/transformers/model_sharing.html)åˆ°[ğŸ¤— Model Hub](https://huggingface.co/models)ã€‚åˆ«äººä¹Ÿå¯ä»¥ç”¨ä½ ä¸Šä¼ çš„æ¨¡å‹ï¼Œé€šè¿‡ç½‘ç»œç›´æ¥ç”¨æ¨¡å‹åå­—å°±èƒ½ç›´æ¥ä¸‹è½½ä¸Šä¼ çš„æ¨¡å‹ã€‚\n",
    "\n",
    "## å‚è€ƒæ–‡çŒ®\n",
    "\n",
    "[4.1-æ–‡æœ¬åˆ†ç±».ipynb](https://github.com/datawhalechina/learn-nlp-with-transformers/blob/main/docs/ç¯‡ç« 4-ä½¿ç”¨Transformersè§£å†³NLPä»»åŠ¡/4.1-æ–‡æœ¬åˆ†ç±».ipynb)\n",
    "\n",
    "[å®˜æ–¹æ–‡æ¡£ï¼štransformers/training](https://huggingface.co/transformers/training.html)\n",
    "\n",
    "[datasetå®˜æ–¹æ–‡æ¡£ï¼šåŠ è½½æœ¬åœ°æ•°æ®](https://huggingface.co/docs/datasets/loading_datasets.html#from-local-files)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
