{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "# BERTå®æˆ˜â€”â€”ï¼ˆ6ï¼‰ç”Ÿæˆä»»åŠ¡-æ‘˜è¦ç”Ÿæˆ\n",
    "\n",
    "## å¼•è¨€\n",
    "\n",
    "è¿™ä¸€ç¯‡å°†ä»‹ç»å¦‚ä½•ä½¿ç”¨ [ğŸ¤— Transformers](https://github.com/huggingface/transformers)ä»£ç åº“ä¸­çš„æ¨¡å‹æ¥è§£å†³**ç”Ÿæˆä»»åŠ¡ä¸­çš„æ‘˜è¦ç”Ÿæˆé—®é¢˜ã€‚**\n",
    "\n",
    "### ä»»åŠ¡ä»‹ç»\n",
    "\n",
    "æ‘˜è¦ç”Ÿæˆï¼Œç”¨ä¸€äº›ç²¾ç‚¼çš„è¯ï¼ˆæ‘˜è¦ï¼‰æ¥æ¦‚æ‹¬æ•´ç‰‡æ–‡ç« çš„å¤§æ„ï¼Œç”¨æˆ·é€šè¿‡è¯»æ–‡æ‘˜å°±å¯ä»¥äº†è§£åˆ°åŸæ–‡è¦è¡¨è¾¾ã€‚\n",
    "\n",
    "ä¸»è¦åˆ†ä¸ºä»¥ä¸‹å‡ ä¸ªéƒ¨åˆ†ï¼š\n",
    "\n",
    "1. æ•°æ®åŠ è½½ï¼›\n",
    "2. æ•°æ®é¢„å¤„ç†ï¼›\n",
    "3. å¾®è°ƒé¢„è®­ç»ƒæ¨¡å‹ï¼šä½¿ç”¨transformerä¸­çš„**`Seq2SeqTrainer`æ¥å£**å¯¹é¢„è®­ç»ƒæ¨¡å‹è¿›è¡Œå¾®è°ƒï¼ˆæ³¨æ„è¿™é‡Œæ˜¯`Seq2SeqTrainer`æ¥å£ï¼Œä¹‹å‰çš„ä»»åŠ¡éƒ½æ˜¯è°ƒç”¨`Trainer`æ¥å£ï¼‰ã€‚\n",
    "\n",
    "### å‰æœŸå‡†å¤‡\n",
    "\n",
    "å®‰è£…ä»¥ä¸‹åº“ï¼š"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pip install datasets transformers rouge-score nltk\n",
    "#transformers==4.9.2\n",
    "#datasets==1.11.0\n",
    "#rouge-score==0.0.4\n",
    "#nltk==3.6.2"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## æ•°æ®åŠ è½½\n",
    "\n",
    "### æ•°æ®é›†ä»‹ç»\n",
    "\n",
    "æˆ‘ä»¬ä½¿ç”¨[XSum dataset](https://arxiv.org/pdf/1808.08745.pdf)æ•°æ®é›†ï¼Œå…¶ä¸­åŒ…å«äº†å¤šç¯‡BBCçš„æ–‡ç« å’Œä¸€å¥å¯¹åº”çš„æ‘˜è¦ã€‚\n",
    "\n",
    "### åŠ è½½æ•°æ®\n",
    "\n",
    "è¯¥æ•°æ®çš„åŠ è½½æ–¹å¼åœ¨transformersåº“ä¸­è¿›è¡Œäº†å°è£…ï¼Œæˆ‘ä»¬å¯ä»¥é€šè¿‡ä»¥ä¸‹è¯­å¥è¿›è¡Œæ•°æ®åŠ è½½ï¼š"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from datasets import load_dataset\n",
    "raw_datasets = load_dataset(\"xsum\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "ç»™å®šä¸€ä¸ªæ•°æ®åˆ‡åˆ†çš„keyï¼ˆtrainã€validationæˆ–è€…testï¼‰å’Œä¸‹æ ‡å³å¯æŸ¥çœ‹æ•°æ®ã€‚"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "raw_datasets[\"train\"][0]\n",
    "# {'document': 'Recent reports have linked some France-based players with returns to Wales.\\n\"I\\'ve always felt - and this is with my rugby hat on now; this is not region or WRU - I\\'d rather spend that money on keeping players in Wales,\" said Davies.\\nThe WRU provides Â£2m to the fund and Â£1.3m comes from the regions.\\nFormer Wales and British and Irish Lions fly-half Davies became WRU chairman on Tuesday 21 October, succeeding deposed David Pickering following governing body elections.\\nHe is now serving a notice period to leave his role as Newport Gwent Dragons chief executive after being voted on to the WRU board in September.\\nDavies was among the leading figures among Dragons, Ospreys, Scarlets and Cardiff Blues officials who were embroiled in a protracted dispute with the WRU that ended in a Â£60m deal in August this year.\\nIn the wake of that deal being done, Davies said the Â£3.3m should be spent on ensuring current Wales-based stars remain there.\\nIn recent weeks, Racing Metro flanker Dan Lydiate was linked with returning to Wales.\\nLikewise the Paris club\\'s scrum-half Mike Phillips and centre Jamie Roberts were also touted for possible returns.\\nWales coach Warren Gatland has said: \"We haven\\'t instigated contact with the players.\\n\"But we are aware that one or two of them are keen to return to Wales sooner rather than later.\"\\nSpeaking to Scrum V on BBC Radio Wales, Davies re-iterated his stance, saying keeping players such as Scarlets full-back Liam Williams and Ospreys flanker Justin Tipuric in Wales should take precedence.\\n\"It\\'s obviously a limited amount of money [available]. The union are contributing 60% of that contract and the regions are putting Â£1.3m in.\\n\"So it\\'s a total pot of just over Â£3m and if you look at the sorts of salaries that the... guys... have been tempted to go overseas for [are] significant amounts of money.\\n\"So if we were to bring the players back, we\\'d probably get five or six players.\\n\"And I\\'ve always felt - and this is with my rugby hat on now; this is not region or WRU - I\\'d rather spend that money on keeping players in Wales.\\n\"There are players coming out of contract, perhaps in the next year or soâ€¦ you\\'re looking at your Liam Williams\\' of the world; Justin Tipuric for example - we need to keep these guys in Wales.\\n\"We actually want them there. They are the ones who are going to impress the young kids, for example.\\n\"They are the sort of heroes that our young kids want to emulate.\\n\"So I would start off [by saying] with the limited pot of money, we have to retain players in Wales.\\n\"Now, if that can be done and there\\'s some spare monies available at the end, yes, let\\'s look to bring players back.\\n\"But it\\'s a cruel world, isn\\'t it?\\n\"It\\'s fine to take the buck and go, but great if you can get them back as well, provided there\\'s enough money.\"\\nBritish and Irish Lions centre Roberts has insisted he will see out his Racing Metro contract.\\nHe and Phillips also earlier dismissed the idea of leaving Paris.\\nRoberts also admitted being hurt by comments in French Newspaper L\\'Equipe attributed to Racing Coach Laurent Labit questioning their effectiveness.\\nCentre Roberts and flanker Lydiate joined Racing ahead of the 2013-14 season while scrum-half Phillips moved there in December 2013 after being dismissed for disciplinary reasons by former club Bayonne.',\n",
    "# 'id': '29750031',\n",
    "# 'summary': 'New Welsh Rugby Union chairman Gareth Davies believes a joint Â£3.3m WRU-regions fund should be used to retain home-based talent such as Liam Williams, not bring back exiled stars.'}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "ä¸‹é¢çš„å‡½æ•°å°†ä»æ•°æ®é›†é‡Œéšæœºé€‰æ‹©å‡ ä¸ªä¾‹å­è¿›è¡Œå±•ç¤ºï¼š"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import datasets\n",
    "import random\n",
    "import pandas as pd\n",
    "from IPython.display import display, HTML\n",
    "\n",
    "def show_random_elements(dataset, num_examples=1):\n",
    "    assert num_examples <= len(dataset), \"Can't pick more elements than there are in the dataset.\"\n",
    "    picks = []\n",
    "    for _ in range(num_examples):\n",
    "        pick = random.randint(0, len(dataset)-1)\n",
    "        while pick in picks:\n",
    "            pick = random.randint(0, len(dataset)-1)\n",
    "        picks.append(pick)\n",
    "    \n",
    "    df = pd.DataFrame(dataset[picks])\n",
    "    for column, typ in dataset.features.items():\n",
    "        if isinstance(typ, datasets.ClassLabel):\n",
    "            df[column] = df[column].transform(lambda i: typ.names[i])\n",
    "    display(HTML(df.to_html()))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "show_random_elements(raw_datasets[\"train\"][1:])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<table border=\"1\" class=\"dataframe\">\n",
    "  <thead>\n",
    "    <tr style=\"text-align: right;\">\n",
    "      <th></th>\n",
    "      <th>document</th>\n",
    "      <th>id</th>\n",
    "      <th>summary</th>\n",
    "    </tr>\n",
    "  </thead>\n",
    "  <tbody>\n",
    "    <tr>\n",
    "      <th>0</th>\n",
    "      <td>Media playback is unsupported on your device\\n18 December 2014 Last updated at 10:28 GMT\\nMalaysia has successfully tackled poverty over the last four decades by drawing on its rich natural resources.\\nAccording to the World Bank, some 49% of Malaysians in 1970 were extremely poor, and that figure has been reduced to 1% today. However, the government's next challenge is to help the lower income group to move up to the middle class, the bank says.\\nUlrich Zahau, the World Bank's Southeast Asia director, spoke to the BBC's Jennifer Pak.</td>\n",
    "      <td>30530533</td>\n",
    "      <td>In Malaysia the \"aspirational\" low-income part of the population is helping to drive economic growth through consumption, according to the World Bank.</td>\n",
    "    </tr>\n",
    "  </tbody>\n",
    "</table>\n",
    "\n",
    "## æ•°æ®é¢„å¤„ç†\n",
    "\n",
    "åœ¨å°†æ•°æ®å–‚å…¥æ¨¡å‹ä¹‹å‰ï¼Œæˆ‘ä»¬éœ€è¦å¯¹æ•°æ®è¿›è¡Œé¢„å¤„ç†ã€‚\n",
    "\n",
    "ä»ç„¶æ˜¯ä¸¤ä¸ªæ•°æ®é¢„å¤„ç†çš„åŸºæœ¬æµç¨‹ï¼š\n",
    "\n",
    "1. åˆ†è¯ï¼›\n",
    "2. è½¬åŒ–æˆå¯¹åº”ä»»åŠ¡è¾“å…¥æ¨¡å‹çš„æ ¼å¼ï¼›\n",
    "\n",
    "`Tokenizer`ç”¨äºä¸Šé¢ä¸¤æ­¥æ•°æ®é¢„å¤„ç†å·¥ä½œï¼š`Tokenizer`é¦–å…ˆå¯¹è¾“å…¥è¿›è¡Œtokenizeï¼Œç„¶åå°†tokensè½¬åŒ–ä¸ºé¢„æ¨¡å‹ä¸­éœ€è¦å¯¹åº”çš„token IDï¼Œå†è½¬åŒ–ä¸ºæ¨¡å‹éœ€è¦çš„è¾“å…¥æ ¼å¼ã€‚\n",
    "\n",
    "### åˆå§‹åŒ–Tokenizer\n",
    "\n",
    "[ä¹‹å‰çš„åšå®¢](https://ifwind.github.io/2021/08/26/BERT%E5%AE%9E%E6%88%98%E2%80%94%E2%80%94%EF%BC%881%EF%BC%89%E6%96%87%E6%9C%AC%E5%88%86%E7%B1%BB/#%E5%88%9D%E5%A7%8B%E5%8C%96Tokenizer)å·²ç»ä»‹ç»äº†ä¸€äº›Tokenizerçš„å†…å®¹ï¼Œå¹¶åšäº†Tokenizeråˆ†è¯çš„ç¤ºä¾‹ï¼Œè¿™é‡Œä¸å†é‡å¤ã€‚`use_fast=True`æŒ‡å®šä½¿ç”¨fastç‰ˆæœ¬çš„tokenizerã€‚\n",
    "\n",
    "æˆ‘ä»¬ä½¿ç”¨[`t5-small`](https://huggingface.co/t5-small)æ¨¡å‹checkpointæ¥åšè¯¥ä»»åŠ¡ã€‚"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import AutoTokenizer\n",
    "model_checkpoint = \"t5-small\"\n",
    "tokenizer = AutoTokenizer.from_pretrained(model_checkpoint, use_fast=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### è½¬åŒ–æˆå¯¹åº”ä»»åŠ¡è¾“å…¥æ¨¡å‹çš„æ ¼å¼\n",
    "\n",
    "æ¨¡å‹çš„è¾“å…¥ä¸ºå¾…ç¿»è¯‘çš„å¥å­ã€‚\n",
    "\n",
    "**æ³¨æ„ï¼šä¸ºäº†ç»™æ¨¡å‹å‡†å¤‡å¥½ç¿»è¯‘çš„targetsï¼Œä½¿ç”¨`as_target_tokenizer`æ¥ä¸ºtargetsè®¾ç½®tokenizerï¼š**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "with tokenizer.as_target_tokenizer():\n",
    "    print(tokenizer([\"Hello, this one sentence!\", \"This is another sentence.\"]))\n",
    "#{'input_ids': [[8774, 6, 48, 80, 7142, 55, 1], [100, 19, 430, 7142, 5, 1]], 'attention_mask': [[1, 1, 1, 1, 1, 1, 1], [1, 1, 1, 1, 1, 1]]}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**å¦‚æœä½¿ç”¨çš„æ˜¯T5é¢„è®­ç»ƒæ¨¡å‹çš„checkpointsï¼ˆæ¯”å¦‚æˆ‘ä»¬è¿™é‡Œç”¨çš„t5-smallï¼‰ï¼Œéœ€è¦å¯¹ç‰¹æ®Šçš„å‰ç¼€è¿›è¡Œæ£€æŸ¥ã€‚T5ä½¿ç”¨ç‰¹æ®Šçš„å‰ç¼€ï¼ˆ`\"summarize: \"`ï¼‰æ¥å‘Šè¯‰æ¨¡å‹å…·ä½“è¦åšçš„ä»»åŠ¡**ï¼Œå…·ä½“å‰ç¼€ä¾‹å­å¦‚ä¸‹ï¼š"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "if model_checkpoint in [\"t5-small\", \"t5-base\", \"t5-larg\", \"t5-3b\", \"t5-11b\"]:\n",
    "    prefix = \"summarize: \"\n",
    "else:\n",
    "    prefix = \"\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "ç°åœ¨æˆ‘ä»¬å¯ä»¥æŠŠä¸Šé¢çš„å†…å®¹æ”¾åœ¨ä¸€èµ·ç»„æˆé¢„å¤„ç†å‡½æ•°`preprocess_function`ã€‚å¯¹æ ·æœ¬è¿›è¡Œé¢„å¤„ç†çš„æ—¶å€™ï¼Œ**ä½¿ç”¨`truncation=True`å‚æ•°æ¥ç¡®ä¿è¶…é•¿æ–‡æœ¬è¢«æˆªæ–­ã€‚é»˜è®¤æƒ…å†µä¸‹ï¼Œå¯¹ä¸æ¯”è¾ƒçŸ­çš„å¥å­ä¼šè‡ªåŠ¨paddingã€‚**`max_input_length`æ§åˆ¶äº†è¾“å…¥æ–‡æœ¬çš„é•¿åº¦ï¼Œ`max_target_length`æ§åˆ¶äº†æ‘˜è¦çš„æœ€é•¿é•¿åº¦ã€‚"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "max_input_length = 1024\n",
    "max_target_length = 128\n",
    "\n",
    "def preprocess_function(examples):\n",
    "    inputs = [prefix + doc for doc in examples[\"document\"]]\n",
    "    model_inputs = tokenizer(inputs, max_length=max_input_length, truncation=True)\n",
    "\n",
    "    # Setup the tokenizer for targets\n",
    "    with tokenizer.as_target_tokenizer():\n",
    "        labels = tokenizer(examples[\"summary\"], max_length=max_target_length, truncation=True)\n",
    "\n",
    "    model_inputs[\"labels\"] = labels[\"input_ids\"]\n",
    "    return model_inputs"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "ä»¥ä¸Šçš„é¢„å¤„ç†å‡½æ•°å¯ä»¥å¤„ç†ä¸€ä¸ªæ ·æœ¬ï¼Œä¹Ÿå¯ä»¥å¤„ç†å¤šä¸ªæ ·æœ¬exapmlesã€‚å¦‚æœæ˜¯å¤„ç†å¤šä¸ªæ ·æœ¬ï¼Œåˆ™è¿”å›çš„æ˜¯å¤šä¸ªæ ·æœ¬è¢«é¢„å¤„ç†ä¹‹åçš„ç»“æœlistã€‚\n",
    "\n",
    "æ¥ä¸‹æ¥**ä½¿ç”¨mapå‡½æ•°**å¯¹æ•°æ®é›†**datasetsé‡Œé¢ä¸‰ä¸ªæ ·æœ¬é›†åˆçš„æ‰€æœ‰æ ·æœ¬è¿›è¡Œé¢„å¤„ç†ï¼Œ**å°†é¢„å¤„ç†å‡½æ•°`preprocess_function`åº”ç”¨åˆ°ï¼ˆmap)æ‰€æœ‰æ ·æœ¬ä¸Šã€‚å‚æ•°`batched=True`å¯ä»¥æ‰¹é‡å¯¹æ–‡æœ¬è¿›è¡Œç¼–ç ã€‚è¿™æ˜¯ä¸ºäº†å……åˆ†åˆ©ç”¨å‰é¢åŠ è½½fast_tokenizerçš„ä¼˜åŠ¿ï¼Œå®ƒå°†ä½¿ç”¨å¤šçº¿ç¨‹å¹¶å‘åœ°å¤„ç†æ‰¹ä¸­çš„æ–‡æœ¬ã€‚"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "tokenized_datasets = raw_datasets.map(preprocess_function, batched=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## å¾®è°ƒé¢„è®­ç»ƒæ¨¡å‹\n",
    "\n",
    "æ•°æ®å·²ç»å‡†å¤‡å¥½äº†ï¼Œæˆ‘ä»¬éœ€è¦ä¸‹è½½å¹¶åŠ è½½é¢„è®­ç»ƒæ¨¡å‹ï¼Œç„¶åå¾®è°ƒé¢„è®­ç»ƒæ¨¡å‹ã€‚\n",
    "\n",
    "### åŠ è½½é¢„è®­ç»ƒæ¨¡å‹\n",
    "\n",
    "åš**seq2seqä»»åŠ¡ï¼Œé‚£ä¹ˆéœ€è¦ä¸€ä¸ªèƒ½è§£å†³è¿™ä¸ªä»»åŠ¡çš„æ¨¡å‹ç±»ã€‚æˆ‘ä»¬ä½¿ç”¨`AutoModelForSeq2SeqLM` è¿™ä¸ªç±»**ã€‚\n",
    "\n",
    "å’Œä¹‹å‰å‡ ç¯‡åšå®¢æåˆ°çš„åŠ è½½æ–¹å¼ç›¸åŒä¸å†èµ˜è¿°ã€‚"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import AutoModelForSeq2SeqLM, \n",
    "model = AutoModelForSeq2SeqLM.from_pretrained(model_checkpoint)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### è®¾å®šè®­ç»ƒå‚æ•°\n",
    "\n",
    "ä¸ºäº†èƒ½å¤Ÿå¾—åˆ°ä¸€ä¸ª**`Seq2SeqTrainer`è®­ç»ƒå·¥å…·**ï¼Œæˆ‘ä»¬è¿˜éœ€è¦**è®­ç»ƒçš„è®¾å®š/å‚æ•° [`Seq2SeqTrainingArguments`](https://huggingface.co/transformers/main_classes/trainer.html#transformers.Seq2SeqTrainingArguments)ã€‚è¿™ä¸ªè®­ç»ƒè®¾å®šåŒ…å«äº†èƒ½å¤Ÿå®šä¹‰è®­ç»ƒè¿‡ç¨‹çš„æ‰€æœ‰å±æ€§**ã€‚\n",
    "\n",
    "ç”±äºæ•°æ®é›†æ¯”è¾ƒå¤§ï¼Œ`Seq2SeqTrainer`è®­ç»ƒæ—¶ä¼šåŒæ—¶ä¸æ–­ä¿å­˜æ¨¡å‹ï¼Œæˆ‘ä»¬ç”¨`save_total_limit=3`å‚æ•°æ§åˆ¶è‡³å¤šä¿å­˜3ä¸ªæ¨¡å‹ã€‚"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import Seq2SeqTrainingArguments\n",
    "batch_size = 16\n",
    "args = Seq2SeqTrainingArguments(\n",
    "    \"test-summarization\",\n",
    "    evaluation_strategy = \"epoch\",\n",
    "    learning_rate=2e-5,\n",
    "    per_device_train_batch_size=batch_size,\n",
    "    per_device_eval_batch_size=batch_size,\n",
    "    weight_decay=0.01,\n",
    "    save_total_limit=3,#è‡³å¤šä¿å­˜3ä¸ªæ¨¡å‹\n",
    "    num_train_epochs=1,\n",
    "    predict_with_generate=True,\n",
    "    fp16=True,\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### æ•°æ®æ”¶é›†å™¨data collator\n",
    "\n",
    "æ¥ä¸‹æ¥éœ€è¦å‘Šè¯‰`Trainer`å¦‚ä½•ä»é¢„å¤„ç†çš„è¾“å…¥æ•°æ®ä¸­æ„é€ batchã€‚æˆ‘ä»¬ä½¿ç”¨æ•°æ®æ”¶é›†å™¨`DataCollatorForSeq2Seq`ï¼Œå°†ç»é¢„å¤„ç†çš„è¾“å…¥åˆ†batchå†æ¬¡å¤„ç†åå–‚ç»™æ¨¡å‹ã€‚"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import DataCollatorForSeq2Seq\n",
    "data_collator = DataCollatorForSeq2Seq(tokenizer, model=model)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### å®šä¹‰è¯„ä¼°æ–¹æ³•\n",
    "\n",
    "æˆ‘ä»¬ä½¿ç”¨`'rouge'`æŒ‡æ ‡ï¼Œåˆ©ç”¨`metric.compute`è®¡ç®—è¯¥æŒ‡æ ‡å¯¹æ¨¡å‹è¿›è¡Œè¯„ä¼°ã€‚\n",
    "\n",
    "ä½¿ç”¨`metric.compute`å¯¹æ¯”predictionså’Œlabelsï¼Œä»è€Œè®¡ç®—å¾—åˆ†ã€‚predictionså’Œlabelséƒ½éœ€è¦æ˜¯ä¸€ä¸ªlistã€‚å…·ä½“æ ¼å¼è§ä¸‹é¢çš„ä¾‹å­ï¼š"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fake_preds = [\"hello there\", \"general kenobi\"]\n",
    "fake_labels = [\"hello there\", \"general kenobi\"]\n",
    "metric.compute(predictions=fake_preds, references=fake_labels)\n",
    "#{'rouge1': AggregateScore(low=Score(precision=1.0, recall=1.0, fmeasure=1.0), mid=Score(precision=1.0, recall=1.0, fmeasure=1.0), high=Score(precision=1.0, recall=1.0, fmeasure=1.0)),\n",
    "# 'rouge2': AggregateScore(low=Score(precision=1.0, recall=1.0, fmeasure=1.0), mid=Score(precision=1.0, recall=1.0, fmeasure=1.0), high=Score(precision=1.0, recall=1.0, fmeasure=1.0)),\n",
    "# 'rougeL': AggregateScore(low=Score(precision=1.0, recall=1.0, fmeasure=1.0), mid=Score(precision=1.0, recall=1.0, fmeasure=1.0), high=Score(precision=1.0, recall=1.0, fmeasure=1.0)),\n",
    "# 'rougeLsum': AggregateScore(low=Score(precision=1.0, recall=1.0, fmeasure=1.0), mid=Score(precision=1.0, recall=1.0, fmeasure=1.0), high=Score(precision=1.0, recall=1.0, fmeasure=1.0))}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "å°†æ¨¡å‹é¢„æµ‹é€å…¥è¯„ä¼°ä¹‹å‰ï¼Œè¿˜éœ€è¦å†™`postprocess_text`å‡½æ•°åšä¸€äº›æ•°æ®åå¤„ç†ã€‚\n",
    "\n",
    "[nltk](http://www.nltk.org/)æ˜¯ä¸€ä¸ªè‡ªç„¶è¯­è¨€å¤„ç†çš„pythonå·¥å…·åŒ…ï¼Œæˆ‘ä»¬è¿™é‡Œç”¨åˆ°äº†å…¶ä¸­ä¸€ä¸ªæŒ‰å¥å­åˆ†å‰²çš„å‡½æ•°`nltk.sent_tokenize()`ã€‚"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import nltk\n",
    "import numpy as np\n",
    "\n",
    "def compute_metrics(eval_pred):\n",
    "    predictions, labels = eval_pred\n",
    "    decoded_preds = tokenizer.batch_decode(predictions, skip_special_tokens=True)\n",
    "    # Replace -100 in the labels as we can't decode them.\n",
    "    labels = np.where(labels != -100, labels, tokenizer.pad_token_id)\n",
    "    decoded_labels = tokenizer.batch_decode(labels, skip_special_tokens=True)\n",
    "    \n",
    "    # Rouge expects a newline after each sentence\n",
    "    decoded_preds = [\"\\n\".join(nltk.sent_tokenize(pred.strip())) for pred in decoded_preds] #æŒ‰å¥å­åˆ†å‰²åæ¢è¡Œç¬¦æ‹¼æ¥\n",
    "    decoded_labels = [\"\\n\".join(nltk.sent_tokenize(label.strip())) for label in decoded_labels]\n",
    "    \n",
    "    result = metric.compute(predictions=decoded_preds, references=decoded_labels, use_stemmer=True)\n",
    "    # Extract a few results\n",
    "    result = {key: value.mid.fmeasure * 100 for key, value in result.items()}\n",
    "    \n",
    "    # Add mean generated length\n",
    "    prediction_lens = [np.count_nonzero(pred != tokenizer.pad_token_id) for pred in predictions]\n",
    "    result[\"gen_len\"] = np.mean(prediction_lens)\n",
    "    \n",
    "    return {k: round(v, 4) for k, v in result.items()}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### å¼€å§‹è®­ç»ƒ\n",
    "\n",
    "å°†æ•°æ®/æ¨¡å‹/å‚æ•°ä¼ å…¥`Trainer`å³å¯ï¼š"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import Seq2SeqTrainer\n",
    "trainer = Seq2SeqTrainer(\n",
    "    model,\n",
    "    args,\n",
    "    train_dataset=tokenized_datasets[\"train\"],\n",
    "    eval_dataset=tokenized_datasets[\"validation\"],\n",
    "    data_collator=data_collator,\n",
    "    tokenizer=tokenizer,\n",
    "    compute_metrics=compute_metrics\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "è°ƒç”¨`train`æ–¹æ³•å¼€å§‹è®­ç»ƒï¼š"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "trainer.train()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## å‚è€ƒæ–‡çŒ®\n",
    "\n",
    "[4.7-ç”Ÿæˆä»»åŠ¡-æ‘˜è¦ç”Ÿæˆ.md](https://github.com/datawhalechina/learn-nlp-with-transformers/blob/main/docs/ç¯‡ç« 4-ä½¿ç”¨Transformersè§£å†³NLPä»»åŠ¡/4.7-ç”Ÿæˆä»»åŠ¡-æ‘˜è¦ç”Ÿæˆ.md)\n",
    "\n",
    "[transformerså®˜æ–¹æ–‡æ¡£](https://huggingface.co/transformers)\n",
    "\n",
    "[BERTç›¸å…³â€”â€”ï¼ˆ7ï¼‰å°†BERTåº”ç”¨åˆ°ä¸‹æ¸¸ä»»åŠ¡](https://ifwind.github.io/2021/08/24/BERTç›¸å…³â€”â€”ï¼ˆ7ï¼‰æŠŠBERTåº”ç”¨åˆ°ä¸‹æ¸¸ä»»åŠ¡/)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
