{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "# BERTå®æˆ˜â€”â€”ï¼ˆ3ï¼‰é—®ç­”ä»»åŠ¡-å¤šé€‰é—®ç­”\n",
    "\n",
    "## å¼•è¨€\n",
    "\n",
    "æˆ‘ä»¬å°†å±•ç¤ºå¦‚ä½•ä½¿ç”¨ [ğŸ¤— Transformers](https://github.com/huggingface/transformers)ä»£ç åº“ä¸­çš„æ¨¡å‹æ¥è§£å†³**é—®ç­”ä»»åŠ¡ä¸­çš„å¤šé€‰é—®ç­”é—®é¢˜**ã€‚\n",
    "\n",
    "### ä»»åŠ¡ä»‹ç»\n",
    "\n",
    "è™½ç„¶å«å¤šé€‰é—®ç­”ï¼Œä½†å®é™…ä¸Š**æ˜¯æŒ‡ç»™å‡ºä¸€ä¸ªé—®é¢˜çš„å¤šä¸ªå¯èƒ½çš„ç­”æ¡ˆï¼ˆå¤‡é€‰é¡¹ï¼‰ï¼Œé€‰å‡ºå…¶ä¸­ä¸€ä¸ªæœ€åˆç†çš„**ï¼Œå…¶å®ç±»ä¼¼äºæˆ‘ä»¬å¹³å¸¸åšçš„å•é€‰é¢˜ã€‚è¯¥ä»»åŠ¡çš„**å®è´¨åŒæ ·æ˜¯åˆ†ç±»ä»»åŠ¡ï¼Œåœ¨å¤šä¸ªå¤‡é€‰é¡¹ä¸­è¿›è¡ŒäºŒåˆ†ç±»**ï¼Œæ‰¾åˆ°ç­”æ¡ˆã€‚\n",
    "\n",
    "æ¯”å¦‚è¾“å…¥ä¸€å¥è¯çš„ä¸ŠåŠå¥ï¼Œç»™å‡ºå‡ ä¸ªååŠå¥çš„å¤‡é€‰é¡¹ï¼Œé€‰å‡ºå“ªä¸ªé€‰é¡¹æ˜¯è¿™ä¸ªä¸ŠåŠå¥çš„ååŠå¥ï¼š"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "è¾“å…¥ï¼š(\"ç¦»ç¦»åŸä¸Šè‰\"ï¼Œ[\"å¤©å®‰é—¨ä¸€æ¸¸\",\"ä¸€å²ä¸€æ¯è£\",\"æ˜¥é£å¹åˆç”Ÿ\"])\n",
    "è¾“å‡ºï¼š1"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "ä¸»è¦åˆ†ä¸ºä»¥ä¸‹å‡ ä¸ªéƒ¨åˆ†ï¼š\n",
    "\n",
    "1. æ•°æ®åŠ è½½\n",
    "2. æ•°æ®é¢„å¤„ç†\n",
    "3. å¾®è°ƒé¢„è®­ç»ƒæ¨¡å‹ï¼šä½¿ç”¨transformerä¸­çš„`Trainer`æ¥å£å¯¹é¢„è®­ç»ƒæ¨¡å‹è¿›è¡Œå¾®è°ƒã€‚\n",
    "\n",
    "### å‰æœŸå‡†å¤‡\n",
    "\n",
    "å®‰è£…ä»¥ä¸‹åº“ï¼š"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pip install datasets transformers\n",
    "#transformers==4.9.2\n",
    "#datasets==1.11.0"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## æ•°æ®åŠ è½½\n",
    "\n",
    "### æ•°æ®é›†ä»‹ç»\n",
    "\n",
    "æˆ‘ä»¬ä½¿ç”¨çš„æ•°æ®é›†æ˜¯[SWAG](https://www.aclweb.org/anthology/D18-1009/)ã€‚SWAGæ˜¯ä¸€ä¸ªå…³äºå¸¸è¯†æ¨ç†çš„æ•°æ®é›†ï¼Œæ¯ä¸ªæ ·æœ¬æè¿°ä¸€ç§æƒ…å†µï¼Œç„¶åç»™å‡ºå››ä¸ªå¯èƒ½çš„é€‰é¡¹ã€‚\n",
    "\n",
    "### åŠ è½½æ•°æ®\n",
    "\n",
    "è¯¥æ•°æ®çš„åŠ è½½æ–¹å¼åœ¨transformersåº“ä¸­è¿›è¡Œäº†å°è£…ï¼Œæˆ‘ä»¬å¯ä»¥é€šè¿‡ä»¥ä¸‹è¯­å¥è¿›è¡Œæ•°æ®åŠ è½½ï¼š"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from datasets import load_dataset\n",
    "datasets = load_dataset(\"swag\", \"regular\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "å¦‚æœä½ ä½¿ç”¨çš„æ˜¯è‡ªå·±çš„æ•°æ®ï¼Œå‚è€ƒ[ç¬¬ä¸€ç¯‡å®æˆ˜åšå®¢ã€å®šä½è¯ï¼šåŠ è½½æ•°æ®ã€‘](https://ifwind.github.io/2021/08/26/BERT%E5%AE%9E%E6%88%98%E2%80%94%E2%80%94%EF%BC%881%EF%BC%89%E6%96%87%E6%9C%AC%E5%88%86%E7%B1%BB/#%E5%8A%A0%E8%BD%BD%E6%95%B0%E6%8D%AE)åŠ è½½è‡ªå·±çš„æ•°æ®ã€‚\n",
    "\n",
    "å¦‚æœä¸Šè¿°ä»£ç æ•°æ®é›†åœ¨ä¸‹è½½è¿‡ç¨‹ä¸­å‡ºç°äº†ä¸€äº›é—®é¢˜ï¼Œå¯ä»¥[æ­¤é“¾æ¥](https://gas.graviti.cn/dataset/datawhale/SWAG\n",
    ")ä¸‹è½½æ•°æ®å¹¶è§£å‹ï¼Œå°†è§£å‹åçš„3ä¸ªcsvæ–‡ä»¶å¤åˆ¶åˆ°ä»£ç ç›®å½•ä¸‹ï¼Œç„¶åç”¨åŠ è½½cacheçš„æ–¹å¼è¿›è¡ŒåŠ è½½ï¼š"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "\n",
    "data_path = '.' #æ•°æ®è·¯å¾„\n",
    "cache_dir = os.path.join(data_path, 'cache')\n",
    "data_files = {'train': os.path.join(data_path, 'train.csv'), 'val': os.path.join(data_path, 'val.csv'), 'test': os.path.join(data_path, 'test.csv')}\n",
    "datasets = load_dataset(data_path, 'regular', data_files=data_files, cache_dir=cache_dir)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "ç»™å®šä¸€ä¸ªæ•°æ®åˆ‡åˆ†çš„keyï¼ˆtrainã€validationæˆ–è€…testï¼‰å’Œä¸‹æ ‡å³å¯æŸ¥çœ‹æ•°æ®ã€‚"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "datasets[\"train\"][0]\n",
    "#{'ending0': 'passes by walking down the street playing their instruments.',\n",
    "# 'ending1': 'has heard approaching them.',\n",
    "# 'ending2': \"arrives and they're outside dancing and asleep.\",\n",
    "# 'ending3': 'turns the lead singer watches the performance.',\n",
    "# 'fold-ind': '3416',\n",
    "# 'gold-source': 'gold',\n",
    "# 'label': 0,\n",
    "# 'sent1': 'Members of the procession walk down the street holding small horn brass instruments.',\n",
    "# 'sent2': 'A drum line',\n",
    "# 'startphrase': 'Members of the procession walk down the street holding small horn brass instruments. A drum line',\n",
    "# 'video-id': 'anetv_jkn6uvmqwh4'}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "ä¸‹é¢çš„å‡½æ•°å°†ä»æ•°æ®é›†é‡Œéšæœºé€‰æ‹©å‡ ä¸ªä¾‹å­è¿›è¡Œå±•ç¤ºï¼š"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from datasets import ClassLabel\n",
    "import random\n",
    "import pandas as pd\n",
    "from IPython.display import display, HTML\n",
    "\n",
    "def show_random_elements(dataset, num_examples=3):\n",
    "    assert num_examples <= len(dataset), \"Can't pick more elements than there are in the dataset.\"\n",
    "    picks = []\n",
    "    for _ in range(num_examples):\n",
    "        pick = random.randint(0, len(dataset)-1)\n",
    "        while pick in picks:\n",
    "            pick = random.randint(0, len(dataset)-1)\n",
    "        picks.append(pick)\n",
    "    \n",
    "    df = pd.DataFrame(dataset[picks])\n",
    "    for column, typ in dataset.features.items():\n",
    "        if isinstance(typ, ClassLabel):\n",
    "            df[column] = df[column].transform(lambda i: typ.names[i])\n",
    "    display(HTML(df.to_html()))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "show_random_elements(datasets[\"train\"])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<table border=\"1\" class=\"dataframe\">\n",
    "  <thead>\n",
    "    <tr style=\"text-align: right;\">\n",
    "      <th></th>\n",
    "      <th>ending0</th>\n",
    "      <th>ending1</th>\n",
    "      <th>ending2</th>\n",
    "      <th>ending3</th>\n",
    "      <th>fold-ind</th>\n",
    "      <th>gold-source</th>\n",
    "      <th>label</th>\n",
    "      <th>sent1</th>\n",
    "      <th>sent2</th>\n",
    "      <th>startphrase</th>\n",
    "      <th>video-id</th>\n",
    "    </tr>\n",
    "  </thead>\n",
    "  <tbody>\n",
    "    <tr>\n",
    "      <th>0</th>\n",
    "      <td>are seated on a field.</td>\n",
    "      <td>are skiing down the slope.</td>\n",
    "      <td>are in a lift.</td>\n",
    "      <td>are pouring out in a man.</td>\n",
    "      <td>16668</td>\n",
    "      <td>gold</td>\n",
    "      <td>1</td>\n",
    "      <td>A man is wiping the skiboard.</td>\n",
    "      <td>Group of people</td>\n",
    "      <td>A man is wiping the skiboard. Group of people</td>\n",
    "      <td>anetv_JmL6BiuXr_g</td>\n",
    "    </tr>\n",
    "    <tr>\n",
    "      <th>1</th>\n",
    "      <td>performs stunts inside a gym.</td>\n",
    "      <td>shows several shopping in the water.</td>\n",
    "      <td>continues his skateboard while talking.</td>\n",
    "      <td>is putting a black bike close.</td>\n",
    "      <td>11424</td>\n",
    "      <td>gold</td>\n",
    "      <td>0</td>\n",
    "      <td>The credits of the video are shown.</td>\n",
    "      <td>A lady</td>\n",
    "      <td>The credits of the video are shown. A lady</td>\n",
    "      <td>anetv_dWyE0o2NetQ</td>\n",
    "    </tr>\n",
    "    <tr>\n",
    "      <th>2</th>\n",
    "      <td>is emerging into the hospital.</td>\n",
    "      <td>are strewn under water at some wreckage.</td>\n",
    "      <td>tosses the wand together and saunters into the marketplace.</td>\n",
    "      <td>swats him upside down.</td>\n",
    "      <td>15023</td>\n",
    "      <td>gen</td>\n",
    "      <td>1</td>\n",
    "      <td>Through his binoculars, someone watches a handful of surfers being rolled up into the wave.</td>\n",
    "      <td>Someone</td>\n",
    "      <td>Through his binoculars, someone watches a handful of surfers being rolled up into the wave. Someone</td>\n",
    "      <td>lsmdc3016_CHASING_MAVERICKS-6791</td>\n",
    "    </tr>\n",
    "    <tr>\n",
    "      <th>3</th>\n",
    "      <td>spies someone sitting below.</td>\n",
    "      <td>opens the fridge and checks out the photo.</td>\n",
    "      <td>puts a little sheepishly.</td>\n",
    "      <td>staggers up to him.</td>\n",
    "      <td>5475</td>\n",
    "      <td>gold</td>\n",
    "      <td>3</td>\n",
    "      <td>He tips it upside down, and its little umbrella falls to the floor.</td>\n",
    "      <td>Back inside, someone</td>\n",
    "      <td>He tips it upside down, and its little umbrella falls to the floor. Back inside, someone</td>\n",
    "      <td>lsmdc1008_Spider-Man2-75503</td>\n",
    "    </tr>\n",
    "  </tbody>\n",
    "</table>\n",
    "\n",
    "å¯ä»¥çœ‹åˆ°ï¼Œæ•°æ®é›†ä¸­çš„æ¯ä¸ªç¤ºä¾‹éƒ½æœ‰ä¸€ä¸ªä¸Šä¸‹æ–‡ï¼Œå®ƒæ˜¯ç”±ç¬¬ä¸€ä¸ªå¥å­(å­—æ®µ`sent1`)å’Œç¬¬äºŒä¸ªå¥å­çš„ç®€ä»‹(å­—æ®µ`sent2`)ç»„æˆï¼Œå¹¶ç»™å‡ºå››ç§ç»“å°¾å¥å­çš„å¤‡é€‰é¡¹(å­—æ®µ`ending0`ï¼Œ `ending1`ï¼Œ `ending2`å’Œ`ending3`)ï¼Œç„¶åè®©æ¨¡å‹ä»ä¸­é€‰æ‹©æ­£ç¡®çš„ä¸€ä¸ª(ç”±å­—æ®µ`label`è¡¨ç¤º)ã€‚\n",
    "\n",
    "ä¸‹é¢çš„å‡½æ•°è®©æˆ‘ä»¬æ›´ç›´è§‚åœ°çœ‹åˆ°ä¸€ä¸ªç¤ºä¾‹:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def show_one(example):\n",
    "    print(f\"Context: {example['sent1']}\")\n",
    "    print(f\"  A - {example['sent2']} {example['ending0']}\")\n",
    "    print(f\"  B - {example['sent2']} {example['ending1']}\")\n",
    "    print(f\"  C - {example['sent2']} {example['ending2']}\")\n",
    "    print(f\"  D - {example['sent2']} {example['ending3']}\")\n",
    "    print(f\"\\nGround truth: option {['A', 'B', 'C', 'D'][example['label']]}\")\n",
    "show_one(datasets[\"train\"][0])\n",
    "#Context: Members of the procession walk down the street holding small horn brass instruments.\n",
    "#  A - A drum line passes by walking down the street playing their instruments.\n",
    "#  B - A drum line has heard approaching them.\n",
    "#  C - A drum line arrives and they're outside dancing and asleep.\n",
    "#  D - A drum line turns the lead singer watches the performance.\n",
    "\n",
    "#Ground truth: option A"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## æ•°æ®é¢„å¤„ç†\n",
    "\n",
    "åœ¨å°†æ•°æ®å–‚å…¥æ¨¡å‹ä¹‹å‰ï¼Œæˆ‘ä»¬éœ€è¦å¯¹æ•°æ®è¿›è¡Œé¢„å¤„ç†ã€‚\n",
    "\n",
    "ä»ç„¶æ˜¯ä¸¤ä¸ªæ•°æ®é¢„å¤„ç†çš„åŸºæœ¬æµç¨‹ï¼š\n",
    "\n",
    "1. åˆ†è¯ï¼›\n",
    "2. è½¬åŒ–æˆå¯¹åº”ä»»åŠ¡è¾“å…¥æ¨¡å‹çš„æ ¼å¼ï¼›\n",
    "\n",
    "`Tokenizer`ç”¨äºä¸Šé¢ä¸¤æ­¥æ•°æ®é¢„å¤„ç†å·¥ä½œï¼š`Tokenizer`é¦–å…ˆå¯¹è¾“å…¥è¿›è¡Œtokenizeï¼Œç„¶åå°†tokensè½¬åŒ–ä¸ºé¢„æ¨¡å‹ä¸­éœ€è¦å¯¹åº”çš„token IDï¼Œå†è½¬åŒ–ä¸ºæ¨¡å‹éœ€è¦çš„è¾“å…¥æ ¼å¼ã€‚\n",
    "\n",
    "### åˆå§‹åŒ–Tokenizer\n",
    "\n",
    "[ä¹‹å‰çš„åšå®¢](https://ifwind.github.io/2021/08/26/BERT%E5%AE%9E%E6%88%98%E2%80%94%E2%80%94%EF%BC%881%EF%BC%89%E6%96%87%E6%9C%AC%E5%88%86%E7%B1%BB/#%E5%88%9D%E5%A7%8B%E5%8C%96Tokenizer)å·²ç»ä»‹ç»äº†ä¸€äº›Tokenizerçš„å†…å®¹ï¼Œå¹¶åšäº†Tokenizeråˆ†è¯çš„ç¤ºä¾‹ï¼Œè¿™é‡Œä¸å†é‡å¤ã€‚`use_fast=True`æŒ‡å®šä½¿ç”¨fastç‰ˆæœ¬çš„tokenizerã€‚"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import AutoTokenizer\n",
    "model_checkpoint = \"bert-base-uncased\"\n",
    "tokenizer = AutoTokenizer.from_pretrained(model_checkpoint, use_fast=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### è½¬åŒ–æˆå¯¹åº”ä»»åŠ¡è¾“å…¥æ¨¡å‹çš„æ ¼å¼\n",
    "\n",
    "è¿™ä¸€ç±»å‹ä»»åŠ¡çš„æ¨¡å‹è¾“å…¥æ˜¯ä»€ä¹ˆå‘¢ï¼Ÿ\n",
    "\n",
    "äº‹å®ä¸Šï¼Œæˆ‘ä»¬åº”è¯¥å°†é—®é¢˜å’Œå¤‡é€‰é¡¹åˆ†åˆ«è¿›è¡Œç»„åˆï¼Œç›¸å½“äºä¸€ä¸ªæ ·æœ¬ä¸º**è¾“å…¥å¤‡é€‰é¡¹ä¸ªæ•°ç›¸åŒçš„å¥å­å¯¹åˆ—è¡¨**ï¼Œå¦‚ä¸‹æ‰€ç¤ºï¼š"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "[(\"Members of the procession walk down the street holding small horn brass instruments.\",\"A drum line passes by walking down the street playing their instruments.\"),\n",
    "(\"Members of the procession walk down the street holding small horn brass instruments.\",\"A drum line has heard approaching them.\"),\n",
    "(\"Members of the procession walk down the street holding small horn brass instruments.\",\"A drum line arrives and they're outside dancing and asleep.\"),\n",
    "(\"Members of the procession walk down the street holding small horn brass instruments.\",\"A drum line turns the lead singer watches the performance.\")]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "ä¹‹å‰å·²ç»ä»‹ç»è¿‡Tokenizerçš„è¾“å…¥å¯ä»¥æ˜¯ä¸€ä¸ªå•å¥ï¼Œä¹Ÿå¯ä»¥æ˜¯ä¸¤ä¸ªå¥å­ã€‚\n",
    "\n",
    "é‚£ä¹ˆæ˜¾ç„¶åœ¨è°ƒç”¨tokenizerä¹‹å‰ï¼Œæˆ‘ä»¬éœ€è¦é¢„å¤„ç†æ•°æ®é›†å…ˆç”Ÿæˆè¾“å…¥Tokenizerçš„æ ·æœ¬ã€‚\n",
    "\n",
    "åœ¨`preprocess_function`å‡½æ•°ä¸­ï¼š\n",
    "\n",
    "1. é¦–å…ˆå°†æ ·æœ¬ä¸­é—®é¢˜å’Œå¤‡é€‰é¡¹åˆ†åˆ«æ”¾åœ¨ä¸¤ä¸ªåµŒå¥—åˆ—è¡¨ï¼ˆä¸¤ä¸ªåµŒå¥—åˆ—è¡¨åˆ†åˆ«å­˜å‚¨äº†æ¯ä¸ªæ ·æœ¬çš„é—®é¢˜å’Œå¤‡é€‰é¡¹ï¼‰ä¸­ï¼›\n",
    "\n",
    "   æ¯”å¦‚ï¼Œe1_sen1è¡¨ç¤ºæ ·æœ¬1çš„é—®é¢˜ï¼ˆç›¸å½“äºè¾“å…¥tokenizerçš„å¥å­1ï¼‰ï¼Œe1_sen2_1è¡¨ç¤ºæ ·æœ¬1çš„å¤‡é€‰é¡¹1ï¼ˆç›¸å½“äºè¾“å…¥tokenizerçš„å¥å­2ï¼‰.....\n",
    "\n",
    "   ```\n",
    "   [[e1_sen1,e1_sen1,e1_sen1,e1_sen1],\n",
    "    [e2_sen1,e2_sen1,e2_sen1,e2_sen1],\n",
    "    [e3_sen1,e3_sen1,e3_sen1,e3_sen1]]\n",
    "    \n",
    "   [[e1_sen2_1,e1_sen2_2,e1_sen2_3,e1_sen2_4],\n",
    "    [e2_sen2_1,e2_sen2_2,e2_sen2_3,e2_sen2_4],\n",
    "    [e3_sen2_1,e3_sen2_2,e3_sen2_3,e3_sen2_4]]\n",
    "   ```\n",
    "\n",
    "2. ç„¶åå°†é—®é¢˜åˆ—è¡¨å’Œå¤‡é€‰é¡¹åˆ—è¡¨æ‹‰å¹³Flatten(ä¸¤ä¸ªåµŒå¥—åˆ—è¡¨å„è‡ªå»æ‰åµŒå¥—)ï¼Œä»¥ä¾¿tokenizerè¿›è¡Œæ‰¹å¤„ç†ï¼Œä»¥é—®é¢˜åˆ—è¡¨ä¸ºä¾‹ï¼š\n",
    "\n",
    "   ```\n",
    "   after flatten->\n",
    "   [e1_sen1,e1_sen1,e1_sen1,e1_sen1,\n",
    "    e2_sen1,e2_sen1,e2_sen1,e2_sen1,\n",
    "    e3_sen1,e3_sen1,e3_sen1,e3_sen1]\n",
    "   after Tokenize->\n",
    "   [e1_tokens1,e1_tokens1,e1_tokens1,e1_tokens1,\n",
    "    e2_tokens1,e2_tokens1,e2_tokens1,e2_tokens1,\n",
    "    e3_tokens1,e3_tokens1,e3_tokens1]\n",
    "   ```\n",
    "\n",
    "3. ç»è¿‡tokenizeråï¼Œå†è½¬å›æ¯ä¸ªæ ·æœ¬æœ‰å¤‡é€‰é¡¹ä¸ªæ•°è¾“å…¥idã€æ³¨æ„åŠ›æ©ç ç­‰ã€‚\n",
    "\n",
    "   ```\n",
    "   after unflatten->\n",
    "   [[e1_tokens1,e1_tokens1,e1_tokens1,e1_tokens1],  \n",
    "    [e2_tokens1,e2_tokens1,e2_tokens1,e2_tokens1]\n",
    "    [e3_tokens1,e3_tokens1,e3_tokens1]]\n",
    "   ```\n",
    "\n",
    "å‚æ•°`truncation=True`ä½¿å¾—æ¯”æ¨¡å‹æ‰€èƒ½æ¥å—æœ€å¤§é•¿åº¦è¿˜é•¿çš„è¾“å…¥è¢«æˆªæ–­ã€‚\n",
    "\n",
    "ä»£ç å¦‚ä¸‹ï¼š"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ending_names = [\"ending0\", \"ending1\", \"ending2\", \"ending3\"]\n",
    "\n",
    "def preprocess_function(examples):\n",
    "    # é¢„å¤„ç†è¾“å…¥tokenizerçš„è¾“å…¥\n",
    "    # Repeat each first sentence four times to go with the four possibilities of second sentences.\n",
    "    first_sentences = [[context] * 4 for context in examples[\"sent1\"]]#æ„é€ å’Œå¤‡é€‰é¡¹ä¸ªæ•°ç›¸åŒçš„é—®é¢˜å¥ï¼Œä¹Ÿæ˜¯tokenizerçš„ç¬¬ä¸€ä¸ªå¥å­\n",
    "    # Grab all second sentences possible for each context.\n",
    "    question_headers = examples[\"sent2\"] #tokenizerçš„ç¬¬äºŒä¸ªå¥å­çš„ä¸ŠåŠå¥\n",
    "    second_sentences = [[f\"{header} {examples[end][i]}\" for end in ending_names] for i, header in enumerate(question_headers)]#æ„é€ ä¸ŠåŠå¥æ‹¼æ¥ä¸‹åŠå¥ä½œä¸ºtokenizerçš„ç¬¬äºŒä¸ªå¥å­ï¼ˆä¹Ÿå°±æ˜¯å¤‡é€‰é¡¹ï¼‰\n",
    "    \n",
    "    # Flatten everything\n",
    "    first_sentences = sum(first_sentences, []) #åˆå¹¶æˆä¸€ä¸ªåˆ—è¡¨æ–¹ä¾¿tokenizerä¸€æ¬¡æ€§å¤„ç†ï¼š[[e1_sen1,e1_sen1,e1_sen1,e1_sen1],[e2_sen1,e2_sen1,e2_sen1,e2_sen1],[e3_sen1,e3_sen1,e3_sen1,e3_sen1]]->[e1_sen1,e1_sen1,e1_sen1,e1_sen1,e2_sen1,e2_sen1,e2_sen1,e2_sen1,e3_sen1,e3_sen1,e3_sen1,e3_sen1]\n",
    "    second_sentences = sum(second_sentences, [])#åˆå¹¶æˆä¸€ä¸ªåˆ—è¡¨æ–¹ä¾¿tokenizerä¸€æ¬¡æ€§å¤„ç†\n",
    "    \n",
    "    # Tokenize\n",
    "    tokenized_examples = tokenizer(first_sentences, second_sentences, truncation=True)\n",
    "    # Un-flatten\n",
    "    # è½¬åŒ–æˆæ¯ä¸ªæ ·æœ¬ï¼ˆä¸€ä¸ªæ ·æœ¬ä¸­åŒ…æ‹¬äº†å››ä¸ªk=[é—®é¢˜1,é—®é¢˜1,é—®é¢˜1,é—®é¢˜1],v=[å¤‡é€‰é¡¹1,å¤‡é€‰é¡¹2,å¤‡é€‰é¡¹3,å¤‡é€‰é¡¹4]ï¼‰\n",
    "    # [e1_tokens1,e1_tokens1,e1_tokens1,e1_tokens1,e2_tokens1,e2_tokens1,e2_tokens1,e2_tokens1,e3_tokens1,e3_tokens1,e3_tokens1,e3_tokens1]->[[e1_tokens1,e1_tokens1,e1_tokens1,e1_tokens1],[e2_tokens1,e2_tokens1,e2_tokens1,e2_tokens1],[e3_tokens1,e3_tokens1,e3_tokens1]]\n",
    "    return {k: [v[i:i+4] for i in range(0, len(v), 4)] for k, v in tokenized_examples.items()}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "ä»¥ä¸Šçš„é¢„å¤„ç†å‡½æ•°å¯ä»¥å¤„ç†ä¸€ä¸ªæ ·æœ¬ï¼Œä¹Ÿå¯ä»¥å¤„ç†å¤šä¸ªæ ·æœ¬exapmlesã€‚å¦‚æœæ˜¯å¤„ç†å¤šä¸ªæ ·æœ¬ï¼Œåˆ™è¿”å›çš„æ˜¯å¤šä¸ªæ ·æœ¬è¢«é¢„å¤„ç†ä¹‹åçš„ç»“æœlistã€‚\n",
    "\n",
    "è®©æˆ‘ä»¬è§£ç ä¸€ä¸‹ç»™å®šç¤ºä¾‹çš„è¾“å…¥ï¼Œå¯ä»¥çœ‹åˆ°ä¸€ä¸ªæ ·æœ¬å¯¹åº”å››ä¸ªé—®é¢˜å’Œå¤‡é€‰é¡¹åˆå¹¶çš„å¥å­ã€‚"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "examples = datasets[\"train\"][:5]\n",
    "features = preprocess_function(examples)\n",
    "idx = 3\n",
    "[tokenizer.decode(features[\"input_ids\"][idx][i]) for i in range(4)]\n",
    "#['[CLS] a drum line passes by walking down the street playing their instruments. [SEP] members of the procession are playing ping pong and celebrating one left each in quick. [SEP]',\n",
    "# '[CLS] a drum line passes by walking down the street playing their instruments. [SEP] members of the procession wait slowly towards the cadets. [SEP]',\n",
    "# '[CLS] a drum line passes by walking down the street playing their instruments. [SEP] members of the procession makes a square call and ends by jumping down into snowy streets where fans begin to take their positions. [SEP]',\n",
    "# '[CLS] a drum line passes by walking down the street playing their instruments. [SEP] members of the procession play and go back and forth hitting the drums while the audience claps for them. [SEP]']"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "æ¥ä¸‹æ¥**ä½¿ç”¨mapå‡½æ•°**å¯¹æ•°æ®é›†**datasetsé‡Œé¢ä¸‰ä¸ªæ ·æœ¬é›†åˆçš„æ‰€æœ‰æ ·æœ¬è¿›è¡Œé¢„å¤„ç†ï¼Œ**å°†é¢„å¤„ç†å‡½æ•°prepare_train_featuresåº”ç”¨åˆ°ï¼ˆmap)æ‰€æœ‰æ ·æœ¬ä¸Šã€‚å‚æ•°`batched=True`å¯ä»¥æ‰¹é‡å¯¹æ–‡æœ¬è¿›è¡Œç¼–ç ã€‚è¿™æ˜¯ä¸ºäº†å……åˆ†åˆ©ç”¨å‰é¢åŠ è½½fast_tokenizerçš„ä¼˜åŠ¿ï¼Œå®ƒå°†ä½¿ç”¨å¤šçº¿ç¨‹å¹¶å‘åœ°å¤„ç†æ‰¹ä¸­çš„æ–‡æœ¬ã€‚"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "tokenized_datasets = datasets.map(preprocess_function, batched=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## å¾®è°ƒé¢„è®­ç»ƒæ¨¡å‹\n",
    "\n",
    "æ•°æ®å·²ç»å‡†å¤‡å¥½äº†ï¼Œæˆ‘ä»¬éœ€è¦ä¸‹è½½å¹¶åŠ è½½é¢„è®­ç»ƒæ¨¡å‹ï¼Œç„¶åå¾®è°ƒé¢„è®­ç»ƒæ¨¡å‹ã€‚\n",
    "\n",
    "### åŠ è½½é¢„è®­ç»ƒæ¨¡å‹\n",
    "\n",
    "åš**å¤šé¡¹é€‰æ‹©ä»»åŠ¡ï¼Œé‚£ä¹ˆéœ€è¦ä¸€ä¸ªèƒ½è§£å†³è¿™ä¸ªä»»åŠ¡çš„æ¨¡å‹ç±»ã€‚æˆ‘ä»¬ä½¿ç”¨`AutoModelForMultipleChoice` è¿™ä¸ªç±»**ã€‚\n",
    "\n",
    "å’Œä¹‹å‰å‡ ç¯‡åšå®¢æåˆ°çš„åŠ è½½æ–¹å¼ç›¸åŒä¸å†èµ˜è¿°ã€‚"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import AutoModelForMultipleChoice\n",
    "model = AutoModelForMultipleChoice.from_pretrained(model_checkpoint)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### è®¾å®šè®­ç»ƒå‚æ•°\n",
    "\n",
    "ä¸ºäº†èƒ½å¤Ÿå¾—åˆ°ä¸€ä¸ª`Trainer`è®­ç»ƒå·¥å…·ï¼Œæˆ‘ä»¬è¿˜éœ€è¦**è®­ç»ƒçš„è®¾å®š/å‚æ•° [`TrainingArguments`](https://huggingface.co/transformers/main_classes/trainer.html#transformers.TrainingArguments)ã€‚è¿™ä¸ªè®­ç»ƒè®¾å®šåŒ…å«äº†èƒ½å¤Ÿå®šä¹‰è®­ç»ƒè¿‡ç¨‹çš„æ‰€æœ‰å±æ€§**ã€‚"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "task='ner'\n",
    "batch_size = 16\n",
    "\n",
    "from transformers import  TrainingArguments\n",
    "\n",
    "args = TrainingArguments( Â  Â \n",
    "    \"test-glue\", Â  Â \n",
    "    evaluation_strategy = \"epoch\", Â  Â \n",
    "    learning_rate=5e-5, Â  Â \n",
    "    per_device_train_batch_size=batch_size, Â  Â \n",
    "    per_device_eval_batch_size=batch_size, Â  Â \n",
    "    num_train_epochs=3, Â  Â \n",
    "    weight_decay=0.01,\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### æ•°æ®æ”¶é›†å™¨data collator\n",
    "\n",
    "æ¥ä¸‹æ¥éœ€è¦å‘Šè¯‰`Trainer`å¦‚ä½•ä»é¢„å¤„ç†çš„è¾“å…¥æ•°æ®ä¸­æ„é€ batchã€‚æˆ‘ä»¬ä½¿ç”¨æ•°æ®æ”¶é›†å™¨data collatorï¼Œå°†ç»é¢„å¤„ç†çš„è¾“å…¥åˆ†batchå†æ¬¡å¤„ç†åå–‚ç»™æ¨¡å‹ã€‚\n",
    "\n",
    "ç”±å‰é¢`preprocess_function`å‡½æ•°çš„è¾“å‡ºæˆ‘ä»¬å¯ä»¥çœ‹åˆ°ï¼Œæ¯ä¸ªæ ·æœ¬éƒ½è¿˜æ²¡æœ‰åšpaddingï¼Œæˆ‘ä»¬åœ¨`data collator`ä¸­æŒ‰ç…§batchå°†æ¯ä¸ªbatchçš„å¥å­paddingåˆ°æ¯ä¸ªbatchæœ€é•¿çš„é•¿åº¦ã€‚æ³¨æ„ï¼Œå› ä¸ºä¸åŒbatchä¸­æœ€é•¿çš„å¥å­ä¸ä¸€å®šéƒ½å’Œæ•´ä¸ªæ•°æ®é›†ä¸­çš„æœ€é•¿å¥å­ä¸€æ ·é•¿ï¼Œä¹Ÿå°±æ˜¯è¯´ä¸æ˜¯æ¯ä¸ªbatchéƒ½éœ€è¦é‚£ä¹ˆé•¿çš„paddingï¼Œæ‰€ä»¥**è¿™é‡Œä¸ç›´æ¥paddingåˆ°æœ€å¤§é•¿åº¦ï¼Œå¯ä»¥æœ‰æ•ˆæå‡è®­ç»ƒæ•ˆç‡**ã€‚\n",
    "\n",
    "ç”±äºtransformersåº“ä¸­æ²¡æœ‰åˆé€‚çš„data collatoræ¥å¤„ç†è¿™æ ·ç‰¹å®šçš„é—®é¢˜ï¼Œæˆ‘ä»¬æ ¹æ®`DataCollatorWithPadding`ç¨ä½œæ”¹åŠ¨æ”¹ç¼–ä¸€ä¸ªåˆé€‚çš„ã€‚æˆ‘åœ¨ä»£ç ä¸­è¡¥å……äº†featureså’Œbatché€æ­¥è½¬åŒ–çš„æ ¼å¼å˜åŒ–è¿‡ç¨‹ï¼š"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from dataclasses import dataclass\n",
    "from transformers.tokenization_utils_base import PreTrainedTokenizerBase, PaddingStrategy\n",
    "from typing import Optional, Union\n",
    "import torch\n",
    "\n",
    "@dataclass\n",
    "class DataCollatorForMultipleChoice:\n",
    "    \"\"\"\n",
    "    Data collator that will dynamically pad the inputs for multiple choice received.\n",
    "    \"\"\"\n",
    "    tokenizer: PreTrainedTokenizerBase\n",
    "    padding: Union[bool, str, PaddingStrategy] = True\n",
    "    max_length: Optional[int] = None\n",
    "    pad_to_multiple_of: Optional[int] = None\n",
    "\n",
    "    def __call__(self, features):\n",
    "        #features:[{'attention_mask':[[],[],...],'input_ids':[[],[],...,'label':_},{'attention_mask':[[],[],...],'input_ids':[[],[],...,'label':_}]\n",
    "        label_name = \"label\" if \"label\" in features[0].keys() else \"labels\"\n",
    "        labels = [feature.pop(label_name) for feature in features] #å°†labelå•ç‹¬å¼¹å‡ºï¼Œfeatures:[{'attention_mask':[[],[],...],'input_ids':[[],[],...]},{'attention_mask':[[],[],...],'input_ids':[[],[],...]}]\n",
    "        batch_size = len(features)\n",
    "        num_choices = len(features[0][\"input_ids\"])\n",
    "        \n",
    "        #feature:{'attention_mask':[[],[],...],'input_ids':[[],[],...]}\n",
    "        #flattened_features:[[{'attention_mask':[],'input_ids':[]},{},{},{}],[]....]\n",
    "        flattened_features = [[{k: v[i] for k, v in feature.items()} for i in range(num_choices)] for feature in features]\n",
    "        #flattened_features:[{'attention_mask':[],'input_ids':[]},{},{},{},{}....]\n",
    "        flattened_features = sum(flattened_features, [])\n",
    "        \n",
    "        # batch: {'attention_mask':[[],[],[],[],[],[],...],'input_ids':[[],[],[],[],[],[],...]}\n",
    "        batch = self.tokenizer.pad(\n",
    "            flattened_features,\n",
    "            padding=self.padding,\n",
    "            max_length=self.max_length,\n",
    "            pad_to_multiple_of=self.pad_to_multiple_of,\n",
    "            return_tensors=\"pt\",\n",
    "        )\n",
    "        \n",
    "        # Un-flatten\n",
    "        # batch: {'attention_mask':[[[],[],[],[]],[[],[],[],[]],[...],...],'input_ids':[[[],[],[],[]],[[],[],[],[]],[...],...]}\n",
    "        batch = {k: v.view(batch_size, num_choices, -1) for k, v in batch.items()}\n",
    "        # Add back labels\n",
    "        # batch: {'attention_mask':[[[],[],[],[]],[[],[],[],[]],[...],...],'input_ids':[[[],[],[],[]],[[],[],[],[]],[...],...],'label':[]}\n",
    "        batch[\"labels\"] = torch.tensor(labels, dtype=torch.int64)\n",
    "        return batch"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "åœ¨ä¸€ä¸ª10ä¸ªæ ·æœ¬çš„batchä¸Šæ£€æŸ¥data collatoræ˜¯å¦æ­£å¸¸å·¥ä½œã€‚\n",
    "\n",
    "**åœ¨è¿™é‡Œæˆ‘ä»¬éœ€è¦ç¡®ä¿featuresä¸­åªæœ‰è¢«æ¨¡å‹æ¥å—çš„è¾“å…¥ç‰¹å¾ï¼ˆä½†è¿™ä¸€æ­¥åœ¨åé¢`Trainer`è‡ªåŠ¨ä¼šç­›é€‰ï¼‰**ï¼š"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "accepted_keys = [\"input_ids\", \"attention_mask\", \"label\"]\n",
    "features = [{k: v for k, v in encoded_datasets[\"train\"][i].items() if k in accepted_keys} for i in range(10)]\n",
    "batch = DataCollatorForMultipleChoice(tokenizer)(features)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "ç„¶åè®©æˆ‘ä»¬æ£€æŸ¥å•ä¸ªæ ·æœ¬æ˜¯å¦å®Œæ•´ï¼Œåˆ©ç”¨ä¹‹å‰çš„show_oneå‡½æ•°è¿›è¡Œå¯¹æ¯”ï¼Œçœ‹æ¥æ²¡é”™ï¼"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "[tokenizer.decode(batch[\"input_ids\"][8][i].tolist()) for i in range(4)]\n",
    "#['[CLS] someone walks over to the radio. [SEP] someone hands her another phone. [SEP] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD]',\n",
    "# '[CLS] someone walks over to the radio. [SEP] someone takes the drink, then holds it. [SEP] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD]',\n",
    "# '[CLS] someone walks over to the radio. [SEP] someone looks off then looks at someone. [SEP] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD]',\n",
    "# '[CLS] someone walks over to the radio. [SEP] someone stares blearily down at the floor. [SEP] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD]']\n",
    "\n",
    "show_one(datasets[\"train\"][8])\n",
    "#    Context: Someone walks over to the radio.\n",
    "#      A - Someone hands her another phone.\n",
    "#      B - Someone takes the drink, then holds it.\n",
    "#      C - Someone looks off then looks at someone.\n",
    "#      D - Someone stares blearily down at the floor.\n",
    "#    \n",
    "#    Ground truth: option D"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### å®šä¹‰è¯„ä¼°æ–¹æ³•\n",
    "\n",
    "æˆ‘ä»¬ä½¿ç”¨`'accuracy'`å¯¹æ¨¡å‹è¿›è¡Œè¯„ä¼°ã€‚\n",
    "\n",
    "éœ€è¦å®šä¹‰ä¸€ä¸ªå‡½æ•°è®¡ç®—è¿”å›ç²¾åº¦ï¼Œå–é¢„æµ‹logitsçš„argmaxå¾—åˆ°é¢„æµ‹æ ‡ç­¾predsï¼Œå’Œground_truthè¿›è¡Œè¿›è¡Œå¯¹æ¯”ï¼Œè®¡ç®—ç²¾åº¦ï¼š"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "from datasets import load_metric\n",
    "def compute_metrics(eval_predictions):\n",
    "    predictions, label_ids = eval_predictions\n",
    "    preds = np.argmax(predictions, axis=1)\n",
    "    return {\"accuracy\": (preds == label_ids).astype(np.float32).mean().item()}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### å¼€å§‹è®­ç»ƒ\n",
    "\n",
    "å°†æ•°æ®/æ¨¡å‹/å‚æ•°ä¼ å…¥`Trainer`å³å¯ï¼š"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import  Trainer\n",
    "trainer = Trainer(\n",
    "    model,\n",
    "    args,\n",
    "    train_dataset=encoded_datasets[\"train\"],\n",
    "    eval_dataset=encoded_datasets[\"validation\"],\n",
    "    tokenizer=tokenizer,\n",
    "    data_collator=DataCollatorForMultipleChoice(tokenizer),\n",
    "    compute_metrics=compute_metrics,\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "è°ƒç”¨`train`æ–¹æ³•å¼€å§‹è®­ç»ƒï¼š"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "trainer.train()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## å‚è€ƒæ–‡çŒ®\n",
    "\n",
    "[4.4-é—®ç­”ä»»åŠ¡-å¤šé€‰é—®ç­”.md](https://github.com/datawhalechina/learn-nlp-with-transformers/blob/main/docs/ç¯‡ç« 4-ä½¿ç”¨Transformersè§£å†³NLPä»»åŠ¡/4.4-é—®ç­”ä»»åŠ¡-å¤šé€‰é—®ç­”.md)\n",
    "\n",
    "[BERTå®æˆ˜â€”â€”ï¼ˆ1ï¼‰æ–‡æœ¬åˆ†ç±»](https://ifwind.github.io/2021/08/26/BERT%E5%AE%9E%E6%88%98%E2%80%94%E2%80%94%EF%BC%881%EF%BC%89%E6%96%87%E6%9C%AC%E5%88%86%E7%B1%BB/)\n",
    "\n",
    "[transformerså®˜æ–¹æ–‡æ¡£](https://huggingface.co/transformers)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
